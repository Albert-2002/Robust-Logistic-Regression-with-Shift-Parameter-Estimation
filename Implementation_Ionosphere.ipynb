{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names based on the UCI Machine Learning Repository description\n",
    "column_names = [f'feature_{i}' for i in range(34)] + ['label']\n",
    "\n",
    "file_path = \"C:/Users/91959/Desktop/CODE/Robust-Logistic-Regression-with-Shift-Parameter-Estimation/Robust Logistic Regression [DATA DIRECTORY]/Linear Case/ionosphere/ionosphere.data\"\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(df[i].value_counts())\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a single column\n",
    "df = df.drop('feature_1', axis=1)  # axis=1 indicates columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the feature set (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "def introduce_label_noise(y, noise_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Introduces label noise by flipping a percentage of majority class labels to the minority class.\n",
    "\n",
    "    Args:\n",
    "        y (pd.Series): The target variable.\n",
    "        noise_percentage (float): The percentage of majority class labels to flip (e.g., 0.1 for 10%).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The target variable with label noise.\n",
    "    \"\"\"\n",
    "\n",
    "    value_counts = y.value_counts()\n",
    "    majority_class = value_counts.idxmax()\n",
    "    minority_class = value_counts.idxmin()\n",
    "\n",
    "    print(\"Original class distribution:\")\n",
    "    print(value_counts)\n",
    "\n",
    "    majority_indices = y[y == majority_class].index\n",
    "    num_noise = int(len(majority_indices) * noise_percentage)\n",
    "\n",
    "    noise_indices = np.random.choice(majority_indices, num_noise, replace=False)\n",
    "\n",
    "    y_noisy = y.copy()\n",
    "    y_noisy.loc[noise_indices] = minority_class\n",
    "\n",
    "    print(\"\\nClass distribution after introducing noise:\")\n",
    "    print(y_noisy.value_counts())\n",
    "\n",
    "    return y_noisy\n",
    "\n",
    "y = introduce_label_noise(y, noise_percentage=0.2) # x % noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train type:\",type(X_train))\n",
    "print(\"X_test type:\",type(X_test))\n",
    "print(\"y_train type:\",type(y_train))\n",
    "print(\"y_test type:\",type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# ROBUST LOGISTIC REGRESSION WITH SHIFT PARAMETER ESTIMATION\n",
    "# ===========================\n",
    "def train_robust_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with L1 penalty\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * np.sum(np.abs(gamma))\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return theta, gamma\n",
    "\n",
    "# ===========================\n",
    "# PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust logistic regression model.\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# APPLY TO YOUR DATASET\n",
    "# ===========================\n",
    "\n",
    "# Convert Pandas DataFrame to NumPy array\n",
    "# X_train_np = X_train.values\n",
    "# X_test_np = X_test.values\n",
    "\n",
    "# Convert labels from {0,1} to {-1,1} while still in pandas Series form\n",
    "y_train_np = 2 * y_train.values - 1  # Convert 0 → -1 and 1 → 1\n",
    "y_test_np = 2 * y_test.values - 1    # Convert 0 → -1 and 1 → 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train robust logistic regression on your dataset\n",
    "theta, gamma = train_robust_logistic_regression(\n",
    "    X_train, y_train_np, lr=0.1, epochs=2000, tol=1e-6, lambda_=0.1, a=2.0, threshold_type=\"hard\"\n",
    ")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "\n",
    "# Compute misclassification rate\n",
    "misclassification_rate = 1 - accuracy_score(y_test_np, y_pred)\n",
    "\n",
    "print(f\"Test misclassification rate: {misclassification_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# ROBUST LOGISTIC REGRESSION WITH ELASTIC NET\n",
    "# ===========================\n",
    "def train_robust_logistic_regression_elastic(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, alpha=0.5, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation and Elastic Net regularization.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "\n",
    "        # Elastic Net penalty gradient:\n",
    "        l1_grad = alpha * np.sign(theta)  # L1 (Lasso)\n",
    "        l2_grad = (1 - alpha) * theta  # L2 (Ridge)\n",
    "\n",
    "        # Apply Elastic Net regularization\n",
    "        gradient_theta += lambda_ * (l1_grad + l2_grad)\n",
    "\n",
    "        # Update weights\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with Elastic Net penalty\n",
    "        l1_term = alpha * np.sum(np.abs(theta))\n",
    "        l2_term = (1 - alpha) * np.sum(theta**2)\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * (l1_term + l2_term)\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return theta, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# FIT ON PREPROCESSED DATASET\n",
    "# ===========================\n",
    "def fit_and_evaluate_robust_logistic_regression(X_train, X_test, y_train, y_test, lambda_, alpha_elastic, a, threshold_type=\"soft\"):\n",
    "    \"\"\"\n",
    "    Fit the robust logistic regression model on the given dataset and evaluate its performance.\n",
    "    \"\"\"\n",
    "    # # Ensure labels are in {-1, 1}\n",
    "    # y_train = np.where(y_train == 1, 1, -1)\n",
    "    # y_test = np.where(y_test == 1, 1, -1)\n",
    "\n",
    "    # Train model\n",
    "    theta, gamma = train_robust_logistic_regression_elastic(\n",
    "        X_train, y_train, lr=0.1, epochs=2000, tol=1e-6,\n",
    "        lambda_=lambda_, alpha=alpha_elastic, a=a, threshold_type=threshold_type\n",
    "    )\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "\n",
    "    # Compute misclassification rate\n",
    "    misclassification_rate = 1 - accuracy_score(y_test, y_pred)\n",
    "    return misclassification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# USAGE EXAMPLE\n",
    "# ===========================\n",
    "# Assuming X_train, X_test, y_train, y_test are preprocessed and available\n",
    "# Example usage (replace X_train, X_test, y_train, y_test with actual dataset variables)\n",
    "misclassification_rate = fit_and_evaluate_robust_logistic_regression(X_train, X_test, y_train_np, y_test_np,\n",
    "                                        lambda_=0.1, alpha_elastic=0.1, a=2.0, threshold_type=\"hard\")\n",
    "print(f\"Test misclassification rate: {misclassification_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 Iteration Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24528301886792447, 0.18867924528301883, 0.23584905660377353, 0.160377358490566, 0.23584905660377353, 0.18867924528301883, 0.18867924528301883, 0.24528301886792447, 0.160377358490566, 0.18867924528301883, 0.2264150943396226, 0.16981132075471694, 0.17924528301886788, 0.2547169811320755, 0.16981132075471694, 0.13207547169811318, 0.2264150943396226, 0.17924528301886788, 0.21698113207547165, 0.21698113207547165, 0.17924528301886788, 0.16981132075471694, 0.2075471698113207, 0.21698113207547165, 0.16981132075471694, 0.2075471698113207, 0.28301886792452835, 0.18867924528301883, 0.160377358490566, 0.13207547169811318, 0.2264150943396226, 0.2547169811320755, 0.23584905660377353, 0.17924528301886788, 0.2735849056603774, 0.2547169811320755, 0.26415094339622647, 0.2264150943396226, 0.18867924528301883, 0.18867924528301883, 0.2264150943396226, 0.2264150943396226, 0.2075471698113207, 0.23584905660377353, 0.16981132075471694, 0.23584905660377353, 0.2075471698113207, 0.2075471698113207, 0.2075471698113207, 0.16981132075471694, 0.19811320754716977, 0.160377358490566, 0.21698113207547165, 0.2075471698113207, 0.18867924528301883, 0.21698113207547165, 0.2075471698113207, 0.19811320754716977, 0.2264150943396226, 0.17924528301886788, 0.16981132075471694, 0.23584905660377353, 0.23584905660377353, 0.2075471698113207, 0.18867924528301883, 0.13207547169811318, 0.2735849056603774, 0.2075471698113207, 0.17924528301886788, 0.18867924528301883, 0.2075471698113207, 0.17924528301886788, 0.19811320754716977, 0.17924528301886788, 0.19811320754716977, 0.2075471698113207, 0.2075471698113207, 0.18867924528301883, 0.17924528301886788, 0.18867924528301883, 0.17924528301886788, 0.2075471698113207, 0.21698113207547165, 0.21698113207547165, 0.18867924528301883, 0.2264150943396226, 0.2075471698113207, 0.15094339622641506, 0.21698113207547165, 0.2264150943396226, 0.2075471698113207, 0.15094339622641506, 0.13207547169811318, 0.2075471698113207, 0.15094339622641506, 0.23584905660377353, 0.16981132075471694, 0.13207547169811318, 0.24528301886792447, 0.24528301886792447, 0.19811320754716977, 0.23584905660377353, 0.23584905660377353, 0.2264150943396226, 0.21698113207547165, 0.23584905660377353, 0.15094339622641506, 0.2264150943396226, 0.160377358490566, 0.23584905660377353, 0.24528301886792447, 0.160377358490566, 0.2264150943396226, 0.21698113207547165, 0.2264150943396226, 0.24528301886792447, 0.19811320754716977, 0.24528301886792447, 0.19811320754716977, 0.23584905660377353, 0.2075471698113207, 0.17924528301886788, 0.2547169811320755, 0.17924528301886788, 0.17924528301886788, 0.2075471698113207, 0.2075471698113207, 0.18867924528301883, 0.2547169811320755, 0.2075471698113207, 0.17924528301886788, 0.21698113207547165, 0.14150943396226412, 0.16981132075471694, 0.19811320754716977, 0.24528301886792447, 0.16981132075471694, 0.21698113207547165, 0.21698113207547165, 0.2075471698113207, 0.17924528301886788, 0.2075471698113207, 0.15094339622641506, 0.18867924528301883, 0.19811320754716977, 0.19811320754716977, 0.18867924528301883, 0.160377358490566, 0.28301886792452835, 0.2264150943396226, 0.26415094339622647, 0.2264150943396226, 0.160377358490566, 0.2075471698113207, 0.2075471698113207, 0.2264150943396226, 0.2264150943396226, 0.17924528301886788, 0.23584905660377353, 0.18867924528301883, 0.2075471698113207, 0.24528301886792447, 0.15094339622641506, 0.26415094339622647, 0.2075471698113207, 0.2075471698113207, 0.16981132075471694, 0.18867924528301883, 0.17924528301886788, 0.17924528301886788, 0.16981132075471694, 0.17924528301886788, 0.21698113207547165, 0.2075471698113207, 0.24528301886792447, 0.28301886792452835, 0.2264150943396226, 0.2075471698113207, 0.2075471698113207, 0.18867924528301883, 0.18867924528301883, 0.19811320754716977, 0.2264150943396226, 0.160377358490566, 0.16981132075471694, 0.17924528301886788, 0.15094339622641506, 0.24528301886792447, 0.14150943396226412, 0.2264150943396226, 0.2264150943396226, 0.24528301886792447, 0.2264150943396226, 0.26415094339622647, 0.2075471698113207, 0.18867924528301883, 0.160377358490566, 0.19811320754716977, 0.23584905660377353, 0.23584905660377353, 0.21698113207547165, 0.19811320754716977, 0.17924528301886788, 0.2075471698113207, 0.18867924528301883, 0.2075471698113207, 0.2075471698113207, 0.21698113207547165, 0.15094339622641506, 0.18867924528301883, 0.18867924528301883, 0.24528301886792447, 0.23584905660377353, 0.2075471698113207, 0.2075471698113207, 0.2075471698113207, 0.2264150943396226, 0.160377358490566, 0.16981132075471694, 0.24528301886792447, 0.19811320754716977, 0.2547169811320755, 0.2075471698113207, 0.160377358490566, 0.21698113207547165, 0.18867924528301883, 0.24528301886792447, 0.19811320754716977, 0.2264150943396226, 0.2264150943396226, 0.2264150943396226, 0.16981132075471694, 0.15094339622641506, 0.2075471698113207, 0.19811320754716977, 0.19811320754716977, 0.16981132075471694, 0.2075471698113207, 0.2547169811320755, 0.19811320754716977, 0.17924528301886788, 0.2075471698113207, 0.2547169811320755, 0.19811320754716977, 0.19811320754716977, 0.21698113207547165, 0.160377358490566, 0.17924528301886788, 0.2735849056603774, 0.16981132075471694, 0.26415094339622647, 0.2075471698113207, 0.2264150943396226, 0.19811320754716977, 0.2075471698113207, 0.18867924528301883, 0.2075471698113207, 0.2075471698113207, 0.19811320754716977, 0.15094339622641506, 0.19811320754716977, 0.19811320754716977, 0.2547169811320755, 0.21698113207547165, 0.2264150943396226, 0.26415094339622647, 0.16981132075471694, 0.24528301886792447, 0.17924528301886788, 0.160377358490566, 0.160377358490566, 0.2264150943396226, 0.2075471698113207, 0.16981132075471694, 0.2264150943396226, 0.18867924528301883, 0.23584905660377353, 0.19811320754716977, 0.17924528301886788, 0.2264150943396226, 0.19811320754716977, 0.18867924528301883, 0.2075471698113207, 0.28301886792452835, 0.19811320754716977, 0.16981132075471694, 0.26415094339622647, 0.26415094339622647, 0.17924528301886788, 0.2264150943396226, 0.17924528301886788, 0.21698113207547165, 0.16981132075471694, 0.24528301886792447, 0.17924528301886788, 0.19811320754716977, 0.17924528301886788, 0.21698113207547165, 0.2264150943396226, 0.23584905660377353, 0.2075471698113207, 0.24528301886792447, 0.12264150943396224, 0.17924528301886788, 0.2264150943396226, 0.16981132075471694, 0.24528301886792447, 0.17924528301886788, 0.2264150943396226, 0.2264150943396226, 0.15094339622641506, 0.160377358490566, 0.18867924528301883, 0.16981132075471694, 0.17924528301886788, 0.2264150943396226, 0.23584905660377353, 0.19811320754716977, 0.2547169811320755, 0.2264150943396226, 0.26415094339622647, 0.2075471698113207, 0.21698113207547165, 0.24528301886792447, 0.2264150943396226, 0.2264150943396226, 0.15094339622641506, 0.21698113207547165, 0.160377358490566, 0.2547169811320755, 0.24528301886792447, 0.2075471698113207, 0.16981132075471694, 0.2075471698113207, 0.2264150943396226, 0.19811320754716977, 0.17924528301886788, 0.160377358490566, 0.2075471698113207, 0.17924528301886788, 0.19811320754716977, 0.18867924528301883, 0.2075471698113207, 0.17924528301886788, 0.24528301886792447, 0.18867924528301883, 0.19811320754716977, 0.16981132075471694, 0.2075471698113207, 0.21698113207547165, 0.18867924528301883, 0.17924528301886788, 0.2547169811320755, 0.18867924528301883, 0.18867924528301883, 0.19811320754716977, 0.18867924528301883, 0.18867924528301883, 0.21698113207547165, 0.2547169811320755, 0.160377358490566, 0.2075471698113207, 0.2264150943396226, 0.2075471698113207, 0.21698113207547165, 0.19811320754716977, 0.23584905660377353, 0.21698113207547165, 0.2924528301886793, 0.2264150943396226, 0.17924528301886788, 0.160377358490566, 0.14150943396226412, 0.17924528301886788, 0.21698113207547165, 0.15094339622641506, 0.2075471698113207, 0.17924528301886788, 0.19811320754716977, 0.19811320754716977, 0.24528301886792447, 0.26415094339622647, 0.160377358490566, 0.24528301886792447, 0.19811320754716977, 0.2264150943396226, 0.15094339622641506, 0.2075471698113207, 0.15094339622641506, 0.21698113207547165, 0.2075471698113207, 0.13207547169811318, 0.15094339622641506, 0.17924528301886788, 0.2547169811320755, 0.160377358490566, 0.17924528301886788, 0.16981132075471694, 0.21698113207547165, 0.16981132075471694, 0.2075471698113207, 0.17924528301886788, 0.1132075471698113, 0.18867924528301883, 0.15094339622641506, 0.16981132075471694, 0.2264150943396226, 0.19811320754716977, 0.17924528301886788, 0.16981132075471694, 0.2075471698113207, 0.18867924528301883, 0.18867924528301883, 0.17924528301886788, 0.14150943396226412, 0.16981132075471694, 0.17924528301886788, 0.13207547169811318, 0.2264150943396226, 0.18867924528301883, 0.17924528301886788, 0.21698113207547165, 0.21698113207547165, 0.21698113207547165, 0.26415094339622647, 0.17924528301886788, 0.17924528301886788, 0.2264150943396226, 0.21698113207547165, 0.18867924528301883, 0.2075471698113207, 0.2075471698113207, 0.2075471698113207, 0.21698113207547165, 0.2547169811320755, 0.17924528301886788, 0.2264150943396226, 0.28301886792452835, 0.15094339622641506, 0.17924528301886788, 0.24528301886792447, 0.18867924528301883, 0.23584905660377353, 0.16981132075471694, 0.23584905660377353, 0.17924528301886788, 0.19811320754716977, 0.160377358490566, 0.2264150943396226, 0.17924528301886788, 0.21698113207547165, 0.24528301886792447, 0.17924528301886788, 0.24528301886792447, 0.2075471698113207, 0.21698113207547165, 0.2264150943396226, 0.16981132075471694, 0.21698113207547165, 0.18867924528301883, 0.17924528301886788, 0.18867924528301883, 0.2075471698113207, 0.19811320754716977, 0.18867924528301883, 0.2075471698113207, 0.2075471698113207, 0.17924528301886788, 0.160377358490566, 0.160377358490566, 0.21698113207547165, 0.2075471698113207, 0.18867924528301883, 0.16981132075471694, 0.16981132075471694, 0.16981132075471694, 0.17924528301886788, 0.17924528301886788, 0.23584905660377353, 0.17924528301886788, 0.21698113207547165, 0.18867924528301883, 0.160377358490566, 0.18867924528301883, 0.24528301886792447, 0.19811320754716977, 0.18867924528301883, 0.19811320754716977, 0.17924528301886788, 0.17924528301886788, 0.17924528301886788, 0.19811320754716977, 0.2075471698113207, 0.15094339622641506, 0.19811320754716977, 0.23584905660377353, 0.2075471698113207, 0.19811320754716977, 0.160377358490566, 0.21698113207547165]\n",
      "Average misclassification rate over 50 runs L1 Lasso:\n",
      "Average: 0.2023 ± 0.0014 SE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "misclassification_rate_l1_lasso = []\n",
    "\n",
    "# Define column names based on the UCI Machine Learning Repository description\n",
    "column_names = [f'feature_{i}' for i in range(34)] + ['label']\n",
    "\n",
    "file_path = \"C:/Users/91959/Desktop/CODE/Robust-Logistic-Regression-with-Shift-Parameter-Estimation/Robust Logistic Regression [DATA DIRECTORY]/Linear Case/ionosphere/ionosphere.data\"\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "# Drop a single column\n",
    "df = df.drop('feature_1', axis=1)  # axis=1 indicates columns\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Standardize the feature set (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "def introduce_label_noise(y, noise_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Introduces label noise by flipping a percentage of majority class labels to the minority class.\n",
    "\n",
    "    Args:\n",
    "        y (pd.Series): The target variable.\n",
    "        noise_percentage (float): The percentage of majority class labels to flip (e.g., 0.1 for 10%).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The target variable with label noise.\n",
    "    \"\"\"\n",
    "\n",
    "    value_counts = y.value_counts()\n",
    "    majority_class = value_counts.idxmax()\n",
    "    minority_class = value_counts.idxmin()\n",
    "\n",
    "    # print(\"Original class distribution:\")\n",
    "    # print(value_counts)\n",
    "\n",
    "    majority_indices = y[y == majority_class].index\n",
    "    num_noise = int(len(majority_indices) * noise_percentage)\n",
    "\n",
    "    noise_indices = np.random.choice(majority_indices, num_noise, replace=False)\n",
    "\n",
    "    y_noisy = y.copy()\n",
    "    y_noisy.loc[noise_indices] = minority_class\n",
    "\n",
    "    # print(\"\\nClass distribution after introducing noise:\")\n",
    "    # print(y_noisy.value_counts())\n",
    "\n",
    "    return y_noisy\n",
    "\n",
    "y = introduce_label_noise(y, noise_percentage=0.1) # x % noise\n",
    "\n",
    "# Methods for LogReg\n",
    "# ===========================\n",
    "# SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# ROBUST LOGISTIC REGRESSION WITH SHIFT PARAMETER ESTIMATION\n",
    "# ===========================\n",
    "def train_robust_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with L1 penalty\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * np.sum(np.abs(gamma))\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return theta, gamma\n",
    "\n",
    "# ===========================\n",
    "# PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust logistic regression model.\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "def print_average_with_se(data_list):\n",
    "    \"\"\"\n",
    "    Calculates and prints the average of a list with an error bar of ±SE.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): The list of numerical data.\n",
    "    \"\"\"\n",
    "\n",
    "    if not data_list:\n",
    "        print(\"Error: Input list is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        mean = statistics.mean(data_list)\n",
    "        stdev = statistics.stdev(data_list)\n",
    "        se = stdev / math.sqrt(len(data_list))\n",
    "\n",
    "        print(f\"Average: {mean:.4f} ± {se:.4f} SE\")\n",
    "\n",
    "    except statistics.StatisticsError:\n",
    "        print(\"Error: Cannot calculate standard deviation. List must contain at least two elements.\")\n",
    "    except TypeError:\n",
    "        print(\"Error: List elements must be numerical.\")\n",
    "\n",
    "for i in range(500):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=i)\n",
    "    y_train_np = 2 * y_train.values - 1  # Convert 0 → -1 and 1 → 1\n",
    "    y_test_np = 2 * y_test.values - 1    # Convert 0 → -1 and 1 → 1\n",
    "\n",
    "    # Train robust logistic regression on your dataset\n",
    "    theta, gamma = train_robust_logistic_regression(\n",
    "        X_train, y_train_np, lr=0.1, epochs=2000, tol=1e-6, lambda_=0.1, a=2.0, threshold_type=\"hard\"\n",
    "    )\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "\n",
    "    # Compute misclassification rate\n",
    "    misclassification_rate = 1 - accuracy_score(y_test_np, y_pred)\n",
    "    misclassification_rate_l1_lasso.append(misclassification_rate)\n",
    "\n",
    "print(misclassification_rate_l1_lasso)\n",
    "print(f\"Average misclassification rate over 50 runs L1 Lasso:\")\n",
    "print_average_with_se(misclassification_rate_l1_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2075471698113207, 0.14150943396226412, 0.2264150943396226, 0.160377358490566, 0.16981132075471694, 0.15094339622641506, 0.18867924528301883, 0.17924528301886788, 0.14150943396226412, 0.160377358490566, 0.15094339622641506, 0.15094339622641506, 0.160377358490566, 0.15094339622641506, 0.17924528301886788, 0.16981132075471694, 0.18867924528301883, 0.16981132075471694, 0.24528301886792447, 0.1132075471698113, 0.21698113207547165, 0.24528301886792447, 0.14150943396226412, 0.17924528301886788, 0.2075471698113207, 0.1132075471698113, 0.21698113207547165, 0.14150943396226412, 0.160377358490566, 0.2075471698113207, 0.2075471698113207, 0.2264150943396226, 0.21698113207547165, 0.17924528301886788, 0.24528301886792447, 0.17924528301886788, 0.23584905660377353, 0.21698113207547165, 0.19811320754716977, 0.160377358490566, 0.19811320754716977, 0.23584905660377353, 0.17924528301886788, 0.2075471698113207, 0.17924528301886788, 0.19811320754716977, 0.13207547169811318, 0.16981132075471694, 0.2075471698113207, 0.18867924528301883, 0.17924528301886788, 0.17924528301886788, 0.17924528301886788, 0.2075471698113207, 0.2075471698113207, 0.24528301886792447, 0.160377358490566, 0.17924528301886788, 0.19811320754716977, 0.17924528301886788, 0.14150943396226412, 0.18867924528301883, 0.2075471698113207, 0.18867924528301883, 0.2264150943396226, 0.17924528301886788, 0.2075471698113207, 0.17924528301886788, 0.15094339622641506, 0.16981132075471694, 0.16981132075471694, 0.2264150943396226, 0.16981132075471694, 0.160377358490566, 0.2264150943396226, 0.15094339622641506, 0.2075471698113207, 0.2264150943396226, 0.160377358490566, 0.15094339622641506, 0.14150943396226412, 0.160377358490566, 0.21698113207547165, 0.21698113207547165, 0.24528301886792447, 0.16981132075471694, 0.17924528301886788, 0.160377358490566, 0.21698113207547165, 0.17924528301886788, 0.14150943396226412, 0.21698113207547165, 0.14150943396226412, 0.16981132075471694, 0.17924528301886788, 0.23584905660377353, 0.160377358490566, 0.16981132075471694, 0.15094339622641506, 0.2264150943396226, 0.15094339622641506, 0.16981132075471694, 0.160377358490566, 0.160377358490566, 0.160377358490566, 0.19811320754716977, 0.2264150943396226, 0.160377358490566, 0.17924528301886788, 0.16981132075471694, 0.2075471698113207, 0.19811320754716977, 0.19811320754716977, 0.15094339622641506, 0.15094339622641506, 0.21698113207547165, 0.18867924528301883, 0.14150943396226412, 0.2075471698113207, 0.19811320754716977, 0.15094339622641506, 0.18867924528301883, 0.21698113207547165, 0.21698113207547165, 0.19811320754716977, 0.19811320754716977, 0.23584905660377353, 0.17924528301886788, 0.18867924528301883, 0.23584905660377353, 0.160377358490566, 0.160377358490566, 0.2075471698113207, 0.23584905660377353, 0.21698113207547165, 0.17924528301886788, 0.16981132075471694, 0.15094339622641506, 0.2264150943396226, 0.15094339622641506, 0.17924528301886788, 0.23584905660377353, 0.14150943396226412, 0.15094339622641506, 0.18867924528301883, 0.160377358490566, 0.16981132075471694, 0.17924528301886788, 0.2075471698113207, 0.2264150943396226, 0.21698113207547165, 0.2075471698113207, 0.21698113207547165, 0.17924528301886788, 0.13207547169811318, 0.2075471698113207, 0.17924528301886788, 0.14150943396226412, 0.160377358490566, 0.17924528301886788, 0.21698113207547165, 0.19811320754716977, 0.14150943396226412, 0.2547169811320755, 0.160377358490566, 0.23584905660377353, 0.16981132075471694, 0.160377358490566, 0.2075471698113207, 0.160377358490566, 0.160377358490566, 0.19811320754716977, 0.17924528301886788, 0.17924528301886788, 0.160377358490566, 0.18867924528301883, 0.24528301886792447, 0.2264150943396226, 0.16981132075471694, 0.16981132075471694, 0.160377358490566, 0.16981132075471694, 0.17924528301886788, 0.19811320754716977, 0.2075471698113207, 0.18867924528301883, 0.16981132075471694, 0.17924528301886788, 0.160377358490566, 0.21698113207547165, 0.21698113207547165, 0.2264150943396226, 0.17924528301886788, 0.26415094339622647, 0.2075471698113207, 0.19811320754716977, 0.21698113207547165, 0.18867924528301883, 0.18867924528301883, 0.17924528301886788, 0.17924528301886788, 0.160377358490566, 0.12264150943396224, 0.15094339622641506, 0.16981132075471694, 0.18867924528301883, 0.23584905660377353, 0.2547169811320755, 0.24528301886792447, 0.15094339622641506, 0.12264150943396224, 0.26415094339622647, 0.15094339622641506, 0.18867924528301883, 0.2075471698113207, 0.2075471698113207, 0.18867924528301883, 0.2547169811320755, 0.14150943396226412, 0.21698113207547165, 0.17924528301886788, 0.160377358490566, 0.28301886792452835, 0.15094339622641506, 0.2547169811320755, 0.12264150943396224, 0.2264150943396226, 0.16981132075471694, 0.160377358490566, 0.2264150943396226, 0.18867924528301883, 0.16981132075471694, 0.19811320754716977, 0.16981132075471694, 0.23584905660377353, 0.17924528301886788, 0.13207547169811318, 0.14150943396226412, 0.21698113207547165, 0.160377358490566, 0.18867924528301883, 0.19811320754716977, 0.18867924528301883, 0.160377358490566, 0.17924528301886788, 0.21698113207547165, 0.15094339622641506, 0.16981132075471694, 0.19811320754716977, 0.160377358490566, 0.19811320754716977, 0.21698113207547165, 0.17924528301886788, 0.16981132075471694, 0.160377358490566, 0.17924528301886788, 0.13207547169811318, 0.21698113207547165, 0.21698113207547165, 0.18867924528301883, 0.2075471698113207, 0.13207547169811318, 0.18867924528301883, 0.2547169811320755, 0.16981132075471694, 0.17924528301886788, 0.16981132075471694, 0.16981132075471694, 0.15094339622641506, 0.18867924528301883, 0.2075471698113207, 0.17924528301886788, 0.15094339622641506, 0.2075471698113207, 0.26415094339622647, 0.160377358490566, 0.16981132075471694, 0.18867924528301883, 0.12264150943396224, 0.19811320754716977, 0.160377358490566, 0.17924528301886788, 0.2264150943396226, 0.160377358490566, 0.21698113207547165, 0.18867924528301883, 0.18867924528301883, 0.21698113207547165, 0.21698113207547165, 0.160377358490566, 0.160377358490566, 0.19811320754716977, 0.15094339622641506, 0.19811320754716977, 0.17924528301886788, 0.2075471698113207, 0.21698113207547165, 0.14150943396226412, 0.16981132075471694, 0.2075471698113207, 0.2075471698113207, 0.17924528301886788, 0.17924528301886788, 0.160377358490566, 0.15094339622641506, 0.18867924528301883, 0.21698113207547165, 0.16981132075471694, 0.23584905660377353, 0.2075471698113207, 0.16981132075471694, 0.21698113207547165, 0.2264150943396226, 0.23584905660377353, 0.16981132075471694, 0.19811320754716977, 0.24528301886792447, 0.18867924528301883, 0.23584905660377353, 0.2075471698113207, 0.24528301886792447, 0.16981132075471694, 0.13207547169811318, 0.19811320754716977, 0.19811320754716977, 0.23584905660377353, 0.15094339622641506, 0.16981132075471694, 0.160377358490566, 0.2075471698113207, 0.24528301886792447, 0.2264150943396226, 0.160377358490566, 0.2264150943396226, 0.24528301886792447, 0.23584905660377353, 0.18867924528301883, 0.13207547169811318, 0.2075471698113207, 0.14150943396226412, 0.18867924528301883, 0.17924528301886788, 0.160377358490566, 0.16981132075471694, 0.24528301886792447, 0.18867924528301883, 0.17924528301886788, 0.18867924528301883, 0.24528301886792447, 0.21698113207547165, 0.21698113207547165, 0.17924528301886788, 0.21698113207547165, 0.18867924528301883, 0.16981132075471694, 0.21698113207547165, 0.19811320754716977, 0.19811320754716977, 0.16981132075471694, 0.160377358490566, 0.18867924528301883, 0.17924528301886788, 0.24528301886792447, 0.14150943396226412, 0.15094339622641506, 0.2075471698113207, 0.26415094339622647, 0.160377358490566, 0.16981132075471694, 0.17924528301886788, 0.17924528301886788, 0.21698113207547165, 0.17924528301886788, 0.16981132075471694, 0.16981132075471694, 0.19811320754716977, 0.160377358490566, 0.18867924528301883, 0.19811320754716977, 0.16981132075471694, 0.18867924528301883, 0.18867924528301883, 0.14150943396226412, 0.18867924528301883, 0.2075471698113207, 0.24528301886792447, 0.17924528301886788, 0.17924528301886788, 0.13207547169811318, 0.19811320754716977, 0.13207547169811318, 0.19811320754716977, 0.16981132075471694, 0.1132075471698113, 0.18867924528301883, 0.160377358490566, 0.21698113207547165, 0.2075471698113207, 0.14150943396226412, 0.15094339622641506, 0.17924528301886788, 0.15094339622641506, 0.12264150943396224, 0.12264150943396224, 0.18867924528301883, 0.160377358490566, 0.17924528301886788, 0.19811320754716977, 0.2075471698113207, 0.10377358490566035, 0.24528301886792447, 0.17924528301886788, 0.2735849056603774, 0.18867924528301883, 0.2075471698113207, 0.18867924528301883, 0.26415094339622647, 0.21698113207547165, 0.16981132075471694, 0.160377358490566, 0.2264150943396226, 0.14150943396226412, 0.1132075471698113, 0.17924528301886788, 0.160377358490566, 0.18867924528301883, 0.18867924528301883, 0.160377358490566, 0.2264150943396226, 0.17924528301886788, 0.17924528301886788, 0.18867924528301883, 0.13207547169811318, 0.19811320754716977, 0.18867924528301883, 0.15094339622641506, 0.160377358490566, 0.160377358490566, 0.16981132075471694, 0.16981132075471694, 0.19811320754716977, 0.16981132075471694, 0.18867924528301883, 0.2075471698113207, 0.15094339622641506, 0.19811320754716977, 0.14150943396226412, 0.16981132075471694, 0.21698113207547165, 0.24528301886792447, 0.160377358490566, 0.160377358490566, 0.19811320754716977, 0.17924528301886788, 0.24528301886792447, 0.15094339622641506, 0.17924528301886788, 0.15094339622641506, 0.17924528301886788, 0.17924528301886788, 0.15094339622641506, 0.21698113207547165, 0.26415094339622647, 0.16981132075471694, 0.19811320754716977, 0.2075471698113207, 0.19811320754716977, 0.18867924528301883, 0.160377358490566, 0.18867924528301883, 0.14150943396226412, 0.15094339622641506, 0.2264150943396226, 0.17924528301886788, 0.19811320754716977, 0.2075471698113207, 0.16981132075471694, 0.16981132075471694, 0.2264150943396226, 0.2075471698113207, 0.2075471698113207, 0.15094339622641506, 0.160377358490566, 0.17924528301886788, 0.17924528301886788, 0.18867924528301883, 0.18867924528301883, 0.21698113207547165, 0.19811320754716977, 0.17924528301886788, 0.18867924528301883, 0.16981132075471694, 0.23584905660377353, 0.160377358490566, 0.16981132075471694, 0.2264150943396226, 0.18867924528301883, 0.21698113207547165, 0.17924528301886788, 0.19811320754716977]\n",
      "Average misclassification rate over 50 runs Elastic Net:\n",
      "Average: 0.1866 ± 0.0014 SE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "misclassification_rate_elastic_net = []\n",
    "\n",
    "# Define column names based on the UCI Machine Learning Repository description\n",
    "column_names = [f'feature_{i}' for i in range(34)] + ['label']\n",
    "\n",
    "file_path = \"C:/Users/91959/Desktop/CODE/Robust-Logistic-Regression-with-Shift-Parameter-Estimation/Robust Logistic Regression [DATA DIRECTORY]/Linear Case/ionosphere/ionosphere.data\"\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "# Drop a single column\n",
    "df = df.drop('feature_1', axis=1)  # axis=1 indicates columns\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Standardize the feature set (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "def introduce_label_noise(y, noise_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Introduces label noise by flipping a percentage of majority class labels to the minority class.\n",
    "\n",
    "    Args:\n",
    "        y (pd.Series): The target variable.\n",
    "        noise_percentage (float): The percentage of majority class labels to flip (e.g., 0.1 for 10%).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The target variable with label noise.\n",
    "    \"\"\"\n",
    "\n",
    "    value_counts = y.value_counts()\n",
    "    majority_class = value_counts.idxmax()\n",
    "    minority_class = value_counts.idxmin()\n",
    "\n",
    "    # print(\"Original class distribution:\")\n",
    "    # print(value_counts)\n",
    "\n",
    "    majority_indices = y[y == majority_class].index\n",
    "    num_noise = int(len(majority_indices) * noise_percentage)\n",
    "\n",
    "    noise_indices = np.random.choice(majority_indices, num_noise, replace=False)\n",
    "\n",
    "    y_noisy = y.copy()\n",
    "    y_noisy.loc[noise_indices] = minority_class\n",
    "\n",
    "    # print(\"\\nClass distribution after introducing noise:\")\n",
    "    # print(y_noisy.value_counts())\n",
    "\n",
    "    return y_noisy\n",
    "\n",
    "y = introduce_label_noise(y, noise_percentage=0.1) # x % noise\n",
    "\n",
    "# Methods for LogReg\n",
    "# ===========================\n",
    "# SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# ROBUST LOGISTIC REGRESSION WITH ELASTIC NET\n",
    "# ===========================\n",
    "def train_robust_logistic_regression_elastic(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, alpha=0.5, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation and Elastic Net regularization.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "\n",
    "        # Elastic Net penalty gradient:\n",
    "        l1_grad = alpha * np.sign(theta)  # L1 (Lasso)\n",
    "        l2_grad = (1 - alpha) * theta  # L2 (Ridge)\n",
    "\n",
    "        # Apply Elastic Net regularization\n",
    "        gradient_theta += lambda_ * (l1_grad + l2_grad)\n",
    "\n",
    "        # Update weights\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with Elastic Net penalty\n",
    "        l1_term = alpha * np.sum(np.abs(theta))\n",
    "        l2_term = (1 - alpha) * np.sum(theta**2)\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * (l1_term + l2_term)\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return theta, gamma\n",
    "\n",
    "# ===========================\n",
    "# PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust logistic regression model.\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "def print_average_with_se(data_list):\n",
    "    \"\"\"\n",
    "    Calculates and prints the average of a list with an error bar of ±SE.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): The list of numerical data.\n",
    "    \"\"\"\n",
    "\n",
    "    if not data_list:\n",
    "        print(\"Error: Input list is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        mean = statistics.mean(data_list)\n",
    "        stdev = statistics.stdev(data_list)\n",
    "        se = stdev / math.sqrt(len(data_list))\n",
    "\n",
    "        print(f\"Average: {mean:.4f} ± {se:.4f} SE\")\n",
    "\n",
    "    except statistics.StatisticsError:\n",
    "        print(\"Error: Cannot calculate standard deviation. List must contain at least two elements.\")\n",
    "    except TypeError:\n",
    "        print(\"Error: List elements must be numerical.\")\n",
    "\n",
    "for i in range(500):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=i)\n",
    "    y_train_np = 2 * y_train.values - 1  # Convert 0 → -1 and 1 → 1\n",
    "    y_test_np = 2 * y_test.values - 1    # Convert 0 → -1 and 1 → 1\n",
    "\n",
    "    # Train robust logistic regression on your dataset\n",
    "    theta, gamma = train_robust_logistic_regression_elastic(\n",
    "        X_train, y_train_np, lr=0.1, epochs=2000, tol=1e-6, lambda_=0.1, alpha=0.1 ,a=2.0,\n",
    "        threshold_type=\"hard\")\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "\n",
    "    # Compute misclassification rate\n",
    "    misclassification_rate = 1 - accuracy_score(y_test_np, y_pred)\n",
    "    misclassification_rate_elastic_net.append(misclassification_rate)\n",
    "\n",
    "print(misclassification_rate_elastic_net)\n",
    "print(f\"Average misclassification rate over 50 runs Elastic Net:\")\n",
    "print_average_with_se(misclassification_rate_elastic_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
