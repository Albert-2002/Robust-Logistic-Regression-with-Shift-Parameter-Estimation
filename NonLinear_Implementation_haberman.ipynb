{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 Iteration Experiment NON LINEAR\n",
    "\n",
    "L1 Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28260869565217395, 0.3152173913043478, 0.30434782608695654, 0.2934782608695652, 0.3369565217391305, 0.26086956521739135, 0.3586956521739131, 0.30434782608695654, 0.26086956521739135, 0.32608695652173914, 0.23913043478260865, 0.28260869565217395, 0.30434782608695654, 0.2717391304347826, 0.34782608695652173, 0.25, 0.34782608695652173, 0.34782608695652173, 0.28260869565217395, 0.3913043478260869, 0.2717391304347826, 0.3695652173913043, 0.2934782608695652, 0.32608695652173914, 0.2717391304347826, 0.2934782608695652, 0.2717391304347826, 0.28260869565217395, 0.30434782608695654, 0.30434782608695654, 0.23913043478260865, 0.21739130434782605, 0.2934782608695652, 0.3152173913043478, 0.2934782608695652, 0.28260869565217395, 0.30434782608695654, 0.2717391304347826, 0.23913043478260865, 0.23913043478260865, 0.2717391304347826, 0.3369565217391305, 0.3369565217391305, 0.25, 0.32608695652173914, 0.32608695652173914, 0.26086956521739135, 0.3586956521739131, 0.32608695652173914, 0.2934782608695652]\n",
      "Average misclassification rate over 50 runs L1 Lasso:\n",
      "Average: 0.2976 ± 0.0054 SE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "misclassification_rate_l1_lasso = []\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"C:/Users/91959/Desktop/CODE/Robust-Logistic-Regression-with-Shift-Parameter-Estimation/Robust Logistic Regression [DATA DIRECTORY]/Non-Linear Case/haberman/haberman.data\"\n",
    "column_names = [\"Age\", \"Year\", \"Axillary_Nodes\", \"Survival_Status\"]\n",
    "df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "# Convert Survival_Status to binary labels (1 -> 0, 2 -> 1)\n",
    "df[\"Survival_Status\"] = df[\"Survival_Status\"].map({1: 0, 2: 1})\n",
    "\n",
    "# Splitting features and target variable\n",
    "X = df.drop(columns=[\"Survival_Status\"])\n",
    "y = df[\"Survival_Status\"]\n",
    "\n",
    "# Standardize the feature set (zero mean, unit variance)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "X = X.to_numpy()\n",
    "\n",
    "def introduce_label_noise(y, noise_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Introduces label noise by flipping a percentage of majority class labels to the minority class.\n",
    "\n",
    "    Args:\n",
    "        y (pd.Series): The target variable.\n",
    "        noise_percentage (float): The percentage of majority class labels to flip (e.g., 0.1 for 10%).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The target variable with label noise.\n",
    "    \"\"\"\n",
    "\n",
    "    value_counts = y.value_counts()\n",
    "    majority_class = value_counts.idxmax()\n",
    "    minority_class = value_counts.idxmin()\n",
    "\n",
    "    # print(\"Original class distribution:\")\n",
    "    # print(value_counts)\n",
    "\n",
    "    majority_indices = y[y == majority_class].index\n",
    "    num_noise = int(len(majority_indices) * noise_percentage)\n",
    "\n",
    "    noise_indices = np.random.choice(majority_indices, num_noise, replace=False)\n",
    "\n",
    "    y_noisy = y.copy()\n",
    "    y_noisy.loc[noise_indices] = minority_class\n",
    "\n",
    "    # print(\"\\nClass distribution after introducing noise:\")\n",
    "    # print(y_noisy.value_counts())\n",
    "\n",
    "    return y_noisy\n",
    "\n",
    "# ===========================\n",
    "# SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# KERNEL FUNCTION\n",
    "# ===========================\n",
    "def rbf_kernel(X1, X2, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Compute the RBF kernel matrix between X1 and X2.\n",
    "    K(x,y) = exp(-gamma * ||x-y||^2)\n",
    "    \"\"\"\n",
    "    # Compute pairwise squared Euclidean distances\n",
    "    X1_norm = np.sum(X1**2, axis=1).reshape(-1, 1)\n",
    "    X2_norm = np.sum(X2**2, axis=1).reshape(1, -1)\n",
    "\n",
    "    # Use broadcasting to compute the squared distances\n",
    "    distances = X1_norm + X2_norm - 2 * np.dot(X1, X2.T)\n",
    "\n",
    "    # Apply RBF kernel formula\n",
    "    return np.exp(-gamma * distances)\n",
    "\n",
    "# ===========================\n",
    "# THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# ROBUST KERNELIZED LOGISTIC REGRESSION WITH SHIFT PARAMETER ESTIMATION\n",
    "# ===========================\n",
    "def train_robust_kernel_logistic_regression(X, y, kernel_func=rbf_kernel, kernel_param=0.1,\n",
    "                                            lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0,\n",
    "                                            a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust kernelized logistic regression using gradient descent with shift parameter estimation.\n",
    "\n",
    "    Following the screenshot:\n",
    "    - f(x) = β₀ + h(x), where h(x) is in the RKHS induced by kernel K\n",
    "    - By the representer theorem, f(x) = β₀ + Σᵢ αᵢK(x, xᵢ)\n",
    "    - We optimize for α and β₀ instead of θ\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Initialize parameters\n",
    "    beta0 = 0.0\n",
    "    alpha = np.zeros(m)\n",
    "    gamma = np.zeros(m)\n",
    "\n",
    "    # Compute kernel matrix\n",
    "    K = kernel_func(X, X, gamma=kernel_param)\n",
    "\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights (beta0 and alpha) using kernelized logistic regression with offset\n",
    "        z = beta0 + np.dot(K, alpha)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "\n",
    "        # Update beta0\n",
    "        gradient_beta0 = np.mean(h - (y + 1) / 2)\n",
    "        beta0 -= lr * gradient_beta0\n",
    "\n",
    "        # Update alpha\n",
    "        gradient_alpha = np.dot(K, (h - (y + 1) / 2)) / m\n",
    "        alpha -= lr * gradient_alpha\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = beta0 + np.dot(K, alpha)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with L1 penalty\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * np.sum(np.abs(gamma))\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return beta0, alpha, gamma\n",
    "\n",
    "# ===========================\n",
    "# PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_kernel_logistic_regression(X_train, X_test, beta0, alpha, kernel_func=rbf_kernel, kernel_param=0.1):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust kernelized logistic regression model.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training data used to compute the kernel function\n",
    "        X_test: Test data to predict on\n",
    "        beta0: Bias term\n",
    "        alpha: Kernel weights\n",
    "        kernel_func: Kernel function to use\n",
    "        kernel_param: Parameter for the kernel function\n",
    "    \"\"\"\n",
    "    # Compute kernel matrix between test and training data\n",
    "    K_test = kernel_func(X_test, X_train, gamma=kernel_param)\n",
    "\n",
    "    # Compute predictions\n",
    "    z = beta0 + np.dot(K_test, alpha)\n",
    "    probabilities = sigmoid(z)\n",
    "\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "def print_average_with_se(data_list):\n",
    "    \"\"\"\n",
    "    Calculates and prints the average of a list with an error bar of ±SE.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): The list of numerical data.\n",
    "    \"\"\"\n",
    "\n",
    "    if not data_list:\n",
    "        print(\"Error: Input list is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        mean = statistics.mean(data_list)\n",
    "        stdev = statistics.stdev(data_list)\n",
    "        se = stdev / math.sqrt(len(data_list))\n",
    "\n",
    "        print(f\"Average: {mean:.4f} ± {se:.4f} SE\")\n",
    "\n",
    "    except statistics.StatisticsError:\n",
    "        print(\"Error: Cannot calculate standard deviation. List must contain at least two elements.\")\n",
    "    except TypeError:\n",
    "        print(\"Error: List elements must be numerical.\")\n",
    "\n",
    "def cross_validate_parameters(X, y, a_candidates, lambda_candidates, gamma_candidates, threshold_type='soft', n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to select the best parameters a, lambda_, and gamma (kernel parameter).\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_a = None\n",
    "    best_lambda_ = None\n",
    "    best_gamma = None\n",
    "    best_error = float('inf')\n",
    "\n",
    "    for a in a_candidates:\n",
    "        for lambda_ in lambda_candidates:\n",
    "            for gamma in gamma_candidates:\n",
    "                print(f\"a: {a}, lambda: {lambda_}, gamma: {gamma}\")\n",
    "                print('---------------------------------------')\n",
    "                cv_errors = []\n",
    "                for train_index, val_index in kf.split(X):\n",
    "                    X_train, X_val = X[train_index], X[val_index]\n",
    "                    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                    # Convert labels to -1 and 1\n",
    "                    y_train_np = 2 * y_train - 1\n",
    "                    y_val_np = 2 * y_val - 1\n",
    "\n",
    "                    # Train robust kernel logistic regression\n",
    "                    beta0, alpha, gamma_shifts = train_robust_kernel_logistic_regression(\n",
    "                        X_train, y_train_np, kernel_param=gamma, lr=0.1, epochs=2000,\n",
    "                        tol=1e-6, lambda_=lambda_, a=a, threshold_type=threshold_type\n",
    "                    )\n",
    "\n",
    "                    # Predict on validation set\n",
    "                    y_pred = predict_robust_kernel_logistic_regression(X_train, X_val, beta0, alpha, kernel_param=gamma)\n",
    "\n",
    "                    # Compute misclassification rate\n",
    "                    misclassification_rate = 1 - accuracy_score(y_val_np, y_pred)\n",
    "                    cv_errors.append(misclassification_rate)\n",
    "\n",
    "                # Average misclassification rate over the folds\n",
    "                avg_error = np.mean(cv_errors)\n",
    "\n",
    "                # Update best parameters if current combination is better\n",
    "                if avg_error < best_error:\n",
    "                    best_error = avg_error\n",
    "                    best_a = a\n",
    "                    best_lambda_ = lambda_\n",
    "                    best_gamma = gamma\n",
    "\n",
    "    return best_a, best_lambda_, best_gamma, best_error\n",
    "\n",
    "# # Define candidate values for a, lambda_, and gamma (kernel parameter)\n",
    "# a_candidates = [1, 2, 3, 4, 5, float('inf')]\n",
    "# lambda_candidates = [0.01, 0.1, 1.0, 10.0]\n",
    "# gamma_candidates = [0.01, 0.1, 1.0, 10.0]  # RBF kernel parameter\n",
    "\n",
    "# # Perform cross-validation to select the best a, lambda_, and gamma\n",
    "# best_a, best_lambda_, best_gamma, best_error = cross_validate_parameters(\n",
    "#     X, y, a_candidates, lambda_candidates, gamma_candidates, threshold_type='hard'\n",
    "# )\n",
    "\n",
    "# print(f\"Best a: {best_a}, Best lambda_: {best_lambda_}, Best gamma: {best_gamma}, Best CV error: {best_error}\")\n",
    "\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=i)\n",
    "\n",
    "    # Introduce label noise only to the training set\n",
    "    y_train_noisy = introduce_label_noise(y_train, noise_percentage=0.1)  # 10% noise\n",
    "\n",
    "    # Convert labels to -1 and 1 for training and testing sets\n",
    "    y_train_np = 2 * y_train_noisy.values - 1  # Convert 0 → -1 and 1 → 1\n",
    "    y_test_np = 2 * y_test.values - 1          # Convert 0 → -1 and 1 → 1\n",
    "\n",
    "    # Train robust kernel logistic regression on your dataset\n",
    "    # Best a: 1, Best lambda_: 1.0, Best gamma: 0.1, Best CV error: 0.2548387096774193\n",
    "    beta0, alpha, gamma = train_robust_kernel_logistic_regression(\n",
    "        X_train, y_train_np, kernel_param=0.1, lr=0.1, epochs=2000,\n",
    "        tol=1e-6, lambda_=1.0, a=1, threshold_type=\"hard\"\n",
    "    )\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = predict_robust_kernel_logistic_regression(X_train, X_test, beta0, alpha, kernel_param=0.1)\n",
    "\n",
    "    # Compute misclassification rate\n",
    "    misclassification_rate = 1 - accuracy_score(y_test_np, y_pred)\n",
    "    misclassification_rate_l1_lasso.append(misclassification_rate)\n",
    "\n",
    "print(misclassification_rate_l1_lasso)\n",
    "print(f\"Average misclassification rate over 50 runs L1 Lasso:\")\n",
    "print_average_with_se(misclassification_rate_l1_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18478260869565222, 0.30434782608695654, 0.23913043478260865, 0.23913043478260865, 0.23913043478260865, 0.2282608695652174, 0.28260869565217395, 0.25, 0.21739130434782605, 0.25, 0.2282608695652174, 0.19565217391304346, 0.19565217391304346, 0.19565217391304346, 0.2717391304347826, 0.26086956521739135, 0.21739130434782605, 0.21739130434782605, 0.23913043478260865, 0.28260869565217395, 0.2282608695652174, 0.2934782608695652, 0.2717391304347826, 0.2282608695652174, 0.2065217391304348, 0.2282608695652174, 0.2282608695652174, 0.25, 0.21739130434782605, 0.21739130434782605, 0.23913043478260865, 0.21739130434782605, 0.26086956521739135, 0.23913043478260865, 0.2065217391304348, 0.21739130434782605, 0.21739130434782605, 0.25, 0.1630434782608695, 0.2282608695652174, 0.2282608695652174, 0.18478260869565222, 0.3152173913043478, 0.2065217391304348, 0.25, 0.21739130434782605, 0.2717391304347826, 0.32608695652173914, 0.23913043478260865, 0.28260869565217395]\n",
      "Average misclassification rate over 50 runs Elastic Net:\n",
      "Average: 0.2374 ± 0.0048 SE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "misclassification_rate_elastic_net = []\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"C:/Users/91959/Desktop/CODE/Robust-Logistic-Regression-with-Shift-Parameter-Estimation/Robust Logistic Regression [DATA DIRECTORY]/Non-Linear Case/haberman/haberman.data\"\n",
    "column_names = [\"Age\", \"Year\", \"Axillary_Nodes\", \"Survival_Status\"]\n",
    "df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "# Convert Survival_Status to binary labels (1 -> 0, 2 -> 1)\n",
    "df[\"Survival_Status\"] = df[\"Survival_Status\"].map({1: 0, 2: 1})\n",
    "\n",
    "# Splitting features and target variable\n",
    "X = df.drop(columns=[\"Survival_Status\"])\n",
    "y = df[\"Survival_Status\"]\n",
    "\n",
    "# # Standardize the feature set (zero mean, unit variance)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "X = X.to_numpy()\n",
    "\n",
    "def introduce_label_noise(y, noise_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Introduces label noise by flipping a percentage of majority class labels to the minority class.\n",
    "    \"\"\"\n",
    "    value_counts = y.value_counts()\n",
    "    majority_class = value_counts.idxmax()\n",
    "    minority_class = value_counts.idxmin()\n",
    "\n",
    "    majority_indices = y[y == majority_class].index\n",
    "    num_noise = int(len(majority_indices) * noise_percentage)\n",
    "\n",
    "    noise_indices = np.random.choice(majority_indices, num_noise, replace=False)\n",
    "\n",
    "    y_noisy = y.copy()\n",
    "    y_noisy.loc[noise_indices] = minority_class\n",
    "\n",
    "    return y_noisy\n",
    "\n",
    "# Methods for LogReg\n",
    "# ===========================\n",
    "# SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# KERNEL FUNCTION\n",
    "# ===========================\n",
    "def rbf_kernel(X1, X2, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Compute the RBF kernel matrix between X1 and X2.\n",
    "    K(x,y) = exp(-gamma * ||x-y||^2)\n",
    "    \"\"\"\n",
    "    # Compute pairwise squared Euclidean distances\n",
    "    X1_norm = np.sum(X1**2, axis=1).reshape(-1, 1)\n",
    "    X2_norm = np.sum(X2**2, axis=1).reshape(1, -1)\n",
    "\n",
    "    # Use broadcasting to compute the squared distances\n",
    "    distances = X1_norm + X2_norm - 2 * np.dot(X1, X2.T)\n",
    "\n",
    "    # Apply RBF kernel formula\n",
    "    return np.exp(-gamma * distances)\n",
    "\n",
    "# ===========================\n",
    "# THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# ROBUST KERNELIZED LOGISTIC REGRESSION WITH ELASTIC NET\n",
    "# ===========================\n",
    "def train_robust_kernel_logistic_regression_elastic(X, y, kernel_func=rbf_kernel, kernel_param=0.1,\n",
    "                                                    lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0,\n",
    "                                                    alpha=0.5, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust kernelized logistic regression using gradient descent with shift parameter estimation and Elastic Net regularization.\n",
    "\n",
    "    Following the screenshot:\n",
    "    - f(x) = β₀ + h(x), where h(x) is in the RKHS induced by kernel K\n",
    "    - By the representer theorem, f(x) = β₀ + Σᵢ αᵢK(x, xᵢ)\n",
    "    - We optimize for α and β₀ instead of θ with Elastic Net regularization\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Initialize parameters\n",
    "    beta0 = 0.0\n",
    "    alpha_coef = np.zeros(m)  # Renamed from alpha to alpha_coef to avoid confusion with Elastic Net alpha parameter\n",
    "    gamma = np.zeros(m)\n",
    "\n",
    "    # Compute kernel matrix\n",
    "    K = kernel_func(X, X, gamma=kernel_param)\n",
    "\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights (beta0 and alpha_coef) using kernelized logistic regression with offset\n",
    "        z = beta0 + np.dot(K, alpha_coef)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "\n",
    "        # Update beta0\n",
    "        gradient_beta0 = np.mean(h - (y + 1) / 2)\n",
    "        beta0 -= lr * gradient_beta0\n",
    "\n",
    "        # Update alpha_coef with Elastic Net regularization\n",
    "        gradient_alpha_coef = np.dot(K, (h - (y + 1) / 2)) / m\n",
    "\n",
    "        # Elastic Net penalty gradient\n",
    "        l1_grad = alpha * np.sign(alpha_coef)  # L1 (Lasso)\n",
    "        l2_grad = (1 - alpha) * alpha_coef     # L2 (Ridge)\n",
    "\n",
    "        # Apply Elastic Net regularization\n",
    "        gradient_alpha_coef += lambda_ * (l1_grad + l2_grad)\n",
    "\n",
    "        # Update alpha coefficients\n",
    "        alpha_coef -= lr * gradient_alpha_coef\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = beta0 + np.dot(K, alpha_coef)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with Elastic Net penalty\n",
    "        l1_term = alpha * np.sum(np.abs(alpha_coef))\n",
    "        l2_term = (1 - alpha) * np.sum(alpha_coef**2)\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * (l1_term + l2_term)\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return beta0, alpha_coef, gamma\n",
    "\n",
    "# ===========================\n",
    "# PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_kernel_logistic_regression(X_train, X_test, beta0, alpha_coef, kernel_func=rbf_kernel, kernel_param=0.1):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust kernelized logistic regression model.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training data used to compute the kernel function\n",
    "        X_test: Test data to predict on\n",
    "        beta0: Bias term\n",
    "        alpha_coef: Kernel weights\n",
    "        kernel_func: Kernel function to use\n",
    "        kernel_param: Parameter for the kernel function\n",
    "    \"\"\"\n",
    "    # Compute kernel matrix between test and training data\n",
    "    K_test = kernel_func(X_test, X_train, gamma=kernel_param)\n",
    "\n",
    "    # Compute predictions\n",
    "    z = beta0 + np.dot(K_test, alpha_coef)\n",
    "    probabilities = sigmoid(z)\n",
    "\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "def print_average_with_se(data_list):\n",
    "    \"\"\"\n",
    "    Calculates and prints the average of a list with an error bar of ±SE.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        print(\"Error: Input list is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        mean = statistics.mean(data_list)\n",
    "        stdev = statistics.stdev(data_list)\n",
    "        se = stdev / math.sqrt(len(data_list))\n",
    "\n",
    "        print(f\"Average: {mean:.4f} ± {se:.4f} SE\")\n",
    "\n",
    "    except statistics.StatisticsError:\n",
    "        print(\"Error: Cannot calculate standard deviation. List must contain at least two elements.\")\n",
    "    except TypeError:\n",
    "        print(\"Error: List elements must be numerical.\")\n",
    "\n",
    "def cross_validate_parameters(X, y, a_candidates, lambda_candidates, alpha_candidates, gamma_candidates, threshold_type='soft', n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to select the best parameters a, lambda_, alpha, and gamma (kernel parameter).\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_a = None\n",
    "    best_lambda_ = None\n",
    "    best_alpha = None\n",
    "    best_gamma = None\n",
    "    best_error = float('inf')\n",
    "\n",
    "    for a in a_candidates:\n",
    "        for lambda_ in lambda_candidates:\n",
    "            for alpha in alpha_candidates:\n",
    "                for gamma in gamma_candidates:\n",
    "                    print(f\"a: {a}, lambda: {lambda_}, alpha: {alpha}, gamma: {gamma}\")\n",
    "                    print('--------------------------')\n",
    "                    cv_errors = []\n",
    "                    for train_index, val_index in kf.split(X):\n",
    "                        X_train, X_val = X[train_index], X[val_index]\n",
    "                        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                        # Convert labels to -1 and 1\n",
    "                        y_train_np = 2 * y_train - 1\n",
    "                        y_val_np = 2 * y_val - 1\n",
    "\n",
    "                        # Train robust kernel logistic regression with Elastic Net\n",
    "                        beta0, alpha_coef, gamma_shifts = train_robust_kernel_logistic_regression_elastic(\n",
    "                            X_train, y_train_np, kernel_param=gamma, lr=0.1, epochs=2000,\n",
    "                            tol=1e-6, lambda_=lambda_, alpha=alpha, a=a, threshold_type=threshold_type\n",
    "                        )\n",
    "\n",
    "                        # Predict on validation set\n",
    "                        y_pred = predict_robust_kernel_logistic_regression(X_train, X_val, beta0, alpha_coef, kernel_param=gamma)\n",
    "\n",
    "                        # Compute misclassification rate\n",
    "                        misclassification_rate = 1 - accuracy_score(y_val_np, y_pred)\n",
    "                        cv_errors.append(misclassification_rate)\n",
    "\n",
    "                    # Average misclassification rate over the folds\n",
    "                    avg_error = np.mean(cv_errors)\n",
    "\n",
    "                    # Update best parameters if current combination is better\n",
    "                    if avg_error < best_error:\n",
    "                        best_error = avg_error\n",
    "                        best_a = a\n",
    "                        best_lambda_ = lambda_\n",
    "                        best_alpha = alpha\n",
    "                        best_gamma = gamma\n",
    "\n",
    "    return best_a, best_lambda_, best_alpha, best_gamma, best_error\n",
    "\n",
    "# # Define candidate values for a, lambda_, alpha, and gamma\n",
    "# a_candidates = [1, 2, 3, 4, 5, float('inf')]\n",
    "# lambda_candidates = [0.01, 0.1, 1.0, 10.0]\n",
    "# alpha_candidates = [0.1, 0.5, 0.9]  # Values for alpha (balance between L1 and L2)\n",
    "# gamma_candidates = [0.01, 0.1, 1.0, 10.0]  # RBF kernel parameter\n",
    "\n",
    "# # Perform cross-validation to select the best a, lambda_, alpha, and gamma\n",
    "# best_a, best_lambda_, best_alpha, best_gamma, best_error = cross_validate_parameters(\n",
    "#     X, y, a_candidates, lambda_candidates, alpha_candidates, gamma_candidates, threshold_type='hard'\n",
    "# )\n",
    "\n",
    "# print(f\"Best a: {best_a}, Best lambda_: {best_lambda_}, Best alpha: {best_alpha}, Best gamma: {best_gamma}, Best CV error: {best_error}\")\n",
    "\n",
    "# Now run the 50 iteration experiment with the best parameters\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=i)\n",
    "\n",
    "    # Introduce label noise only to the training set\n",
    "    y_train_noisy = introduce_label_noise(y_train, noise_percentage=0.1)  # 10% noise\n",
    "\n",
    "    # Convert labels to -1 and 1 for training and testing sets\n",
    "    y_train_np = 2 * y_train_noisy.values - 1  # Convert 0 → -1 and 1 → 1\n",
    "    y_test_np = 2 * y_test.values - 1          # Convert 0 → -1 and 1 → 1\n",
    "\n",
    "    # Train robust kernel logistic regression on your dataset with the best parameters\n",
    "    beta0, alpha_coef, gamma = train_robust_kernel_logistic_regression_elastic(\n",
    "        X_train, y_train_np, kernel_param=0.01, lr=0.1, epochs=2000,\n",
    "        tol=1e-6, lambda_=0.01, alpha=0.1, a=1, threshold_type=\"hard\"\n",
    "    )\n",
    "\n",
    "    # Predict on test set\n",
    "    # Best a: 1, Best lambda_: 0.01, Best alpha: 0.1, Best gamma: 0.01, Best CV error: 0.26462189317821255\n",
    "    y_pred = predict_robust_kernel_logistic_regression(X_train, X_test, beta0, alpha_coef, kernel_param=0.01)\n",
    "\n",
    "    # Compute misclassification rate\n",
    "    misclassification_rate = 1 - accuracy_score(y_test_np, y_pred)\n",
    "    misclassification_rate_elastic_net.append(misclassification_rate)\n",
    "\n",
    "print(misclassification_rate_elastic_net)\n",
    "print(f\"Average misclassification rate over 50 runs Elastic Net:\")\n",
    "print_average_with_se(misclassification_rate_elastic_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Noise Level': ['5% Noise', '10% Noise'],\n",
    "    'L1 Lasso': [\n",
    "        '0.2798 ± 0.0040 SE',\n",
    "        '0.2983 ± 0.0055 SE'\n",
    "    ],\n",
    "    'Elastic Net': [\n",
    "        '0.2387 ± 0.0043 SE',\n",
    "        '0.2424 ± 0.0044 SE'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the 'Noise Level' column as the index\n",
    "df.set_index('Noise Level', inplace=True)\n",
    "\n",
    "# Write to Excel file\n",
    "df.to_csv('Non-Linear Haberman_Results.csv')\n",
    "\n",
    "print(\"Excel file has been created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
