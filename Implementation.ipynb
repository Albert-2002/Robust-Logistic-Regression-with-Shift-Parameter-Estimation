{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae26cda",
   "metadata": {},
   "source": [
    "### Robust Logistic Regression with Shift Parameter Estimation\n",
    "Python implementation of *Robust Logistic Regression with Shift Parameter Estimation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3ccc3",
   "metadata": {},
   "source": [
    "#### Experiments On Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9e5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d545aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_two_class_data(n, p, alpha):\n",
    "    \"\"\"\n",
    "    Generate synthetic two-class data from a multivariate normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Total number of observations (n/2 for each class).\n",
    "    - p: Number of features.\n",
    "    - alpha: Desired Bayes error rate.\n",
    "\n",
    "    Returns:\n",
    "    - df: A pandas DataFrame containing the generated data with columns 'x1', 'x2', ..., 'xp', 'y'.\n",
    "    \"\"\"\n",
    "    # Calculate c_alpha\n",
    "    z_alpha = norm.ppf(1 - alpha)  # Upper quantile of standard normal distribution\n",
    "    c_alpha = (2 * z_alpha) / np.sqrt(p)\n",
    "\n",
    "    # Mean vectors for the two classes\n",
    "    mu_plus = (c_alpha / 2) * np.ones(p)  # Mean for class y = +1\n",
    "    mu_minus = -(c_alpha / 2) * np.ones(p)  # Mean for class y = -1\n",
    "\n",
    "    # Covariance matrix (identity matrix)\n",
    "    cov = np.eye(p)\n",
    "\n",
    "    # Generate data for each class\n",
    "    n_half = n // 2\n",
    "    X_plus = np.random.multivariate_normal(mu_plus, cov, n_half)  # Class y = +1\n",
    "    X_minus = np.random.multivariate_normal(mu_minus, cov, n_half)  # Class y = -1\n",
    "\n",
    "    # Combine the data\n",
    "    X = np.vstack((X_plus, X_minus))\n",
    "    y = np.hstack((np.ones(n_half), -np.ones(n_half)))\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    columns = [f'x{i+1}' for i in range(p)]\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    df['y'] = y\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa13a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         x1        x2    y\n",
      "0  0.372119 -0.644143  1.0\n",
      "1  0.948267 -0.027580  1.0\n",
      "2  1.774160  0.172777  1.0\n",
      "3  1.567022 -0.619441  1.0\n",
      "4  1.385837 -0.745228  1.0\n"
     ]
    }
   ],
   "source": [
    "n = 200  # Total number of observations\n",
    "p = 2    # Number of features\n",
    "alpha = 0.1  # Desired Bayes error rate\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_two_class_data(n, p, alpha)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81190160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_noise(df, noise_rate):\n",
    "    \"\"\"\n",
    "    Add label noise to the dataset by flipping the labels of a specified percentage of observations\n",
    "    from the -1 class to +1.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The pandas DataFrame containing the dataset.\n",
    "    - noise_rate: The percentage of observations from the -1 class to flip (e.g., 0.05 for 5%).\n",
    "\n",
    "    Returns:\n",
    "    - df_noisy: The DataFrame with added label noise.\n",
    "    \"\"\"\n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_noisy = df.copy()\n",
    "\n",
    "    # Identify the indices of the -1 class\n",
    "    negative_class_indices = df_noisy[df_noisy['y'] == -1].index\n",
    "\n",
    "    # Randomly select a portion of the -1 class to flip\n",
    "    num_to_flip = int(noise_rate * len(negative_class_indices))\n",
    "    flip_indices = np.random.choice(negative_class_indices, size=num_to_flip, replace=False)\n",
    "\n",
    "    # Flip the labels of the selected observations\n",
    "    df_noisy.loc[flip_indices, 'y'] = 1\n",
    "\n",
    "    return df_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0703a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       " 1.0    100\n",
       "-1.0    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4680a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rate = 0.05  # 5% of the -1 class will be flipped to +1\n",
    "df_noisy = add_label_noise(df, noise_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24112bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       " 1.0    105\n",
       "-1.0     95\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noisy['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756e04b",
   "metadata": {},
   "source": [
    "### 100-Repetition Experiment with Normal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test misclassification rate over 100 repetitions: 0.2375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===========================\n",
    "# 1. GENERATE SYNTHETIC DATA\n",
    "# ===========================\n",
    "def generate_two_class_data(n, p, alpha):\n",
    "    \"\"\"\n",
    "    Generate a two-class dataset with a specified Bayes error rate.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Number of observations\n",
    "    - p: Number of features\n",
    "    - alpha: Desired Bayes error rate\n",
    "\n",
    "    Returns:\n",
    "    - df: A DataFrame containing feature columns and target column 'y'.\n",
    "    \"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha)\n",
    "    c_alpha = (2 * z_alpha) / np.sqrt(p)\n",
    "    mu_plus = (c_alpha / 2) * np.ones(p)\n",
    "    mu_minus = -(c_alpha / 2) * np.ones(p)\n",
    "    cov = np.eye(p)\n",
    "    n_half = n // 2\n",
    "    X_plus = np.random.multivariate_normal(mu_plus, cov, n_half)\n",
    "    X_minus = np.random.multivariate_normal(mu_minus, cov, n_half)\n",
    "    X = np.vstack((X_plus, X_minus))\n",
    "    y = np.hstack((np.ones(n_half), -np.ones(n_half)))\n",
    "    df = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(p)])\n",
    "    df['y'] = y\n",
    "    return df\n",
    "\n",
    "# ===========================\n",
    "# 2. ADD LABEL NOISE FUNCTION\n",
    "# ===========================\n",
    "def add_label_noise(df, noise_rate):\n",
    "    \"\"\"\n",
    "    Introduce label noise by flipping a percentage of -1 class labels to +1.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the dataset with class labels\n",
    "    - noise_rate: Proportion of negative class labels (-1) to flip\n",
    "\n",
    "    Returns:\n",
    "    - df_noisy: Modified dataset with noisy labels.\n",
    "    \"\"\"\n",
    "    df_noisy = df.copy()\n",
    "    negative_class_indices = df_noisy[df_noisy['y'] == -1].index\n",
    "    num_to_flip = int(noise_rate * len(negative_class_indices))\n",
    "    flip_indices = np.random.choice(negative_class_indices, size=num_to_flip, replace=False)\n",
    "    df_noisy.loc[flip_indices, 'y'] = 1  # Flip labels from -1 to +1\n",
    "    return df_noisy\n",
    "\n",
    "# ===========================\n",
    "# 3. SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# 4. IMPLEMENT LOGISTIC REGRESSION TRAINING WITH EARLY STOPPING\n",
    "# ===========================\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Train logistic regression using gradient descent with early stopping.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix (including bias column)\n",
    "    - y: Target values (-1 or +1)\n",
    "    - lr: Learning rate\n",
    "    - epochs: Max number of iterations\n",
    "    - tol: Tolerance for early stopping (if loss change < tol, stop training)\n",
    "\n",
    "    Returns:\n",
    "    - theta: Optimized weight parameters\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # Samples, features\n",
    "    theta = np.zeros(n)  # Initialize weights\n",
    "    prev_loss = float('inf')  # Initialize previous loss for tracking\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Compute margin (u = y * f(x))\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "\n",
    "        # Compute predictions using the sigmoid function\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # Compute gradient\n",
    "        gradient = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "\n",
    "        # Update weights\n",
    "        theta -= lr * gradient\n",
    "\n",
    "        # Compute current loss\n",
    "        loss = np.mean(np.log(1 + np.exp(-u)))\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "        # Early stopping condition: If loss improvement is too small, stop training\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "#             print(f\"Early stopping at epoch {epoch}: Loss improvement below {tol}\")\n",
    "            break\n",
    "\n",
    "        prev_loss = loss  # Update previous loss\n",
    "\n",
    "    return theta\n",
    "\n",
    "# ===========================\n",
    "# 5. PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained logistic regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix\n",
    "    - theta: Trained model parameters\n",
    "\n",
    "    Returns:\n",
    "    - Predicted class labels (-1 or 1)\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)  # Convert to {-1,1}\n",
    "\n",
    "# ===========================\n",
    "# 6. CALCULATE AVERAGE TEST MISCLASSIFICATION RATE\n",
    "# ===========================\n",
    "def calculate_avg_misclassification_rate(n, p, alpha, noise_rate, n_repetitions=100):\n",
    "    \"\"\"\n",
    "    Run multiple iterations of training/testing and compute the average misclassification rate.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Number of observations\n",
    "    - p: Number of features\n",
    "    - alpha: Desired Bayes error rate\n",
    "    - noise_rate: Proportion of -1 class flipped to +1\n",
    "    - n_repetitions: Number of iterations for averaging results\n",
    "\n",
    "    Returns:\n",
    "    - avg_misclassification_rate: Mean test misclassification rate over `n_repetitions`.\n",
    "    \"\"\"\n",
    "    misclassification_rates = []\n",
    "\n",
    "    for _ in range(n_repetitions):\n",
    "        # Step 1: Generate clean dataset\n",
    "        df = generate_two_class_data(n, p, alpha)\n",
    "\n",
    "        # Step 2: Add label noise\n",
    "        df_noisy = add_label_noise(df, noise_rate)\n",
    "\n",
    "        # Step 3: Split into training and test sets\n",
    "        X = df_noisy.drop(columns=['y']).values\n",
    "        y = df_noisy['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Step 4: Train logistic regression with early stopping\n",
    "        theta = train_logistic_regression(X_train, y_train, lr=0.1, epochs=2000, tol=1e-6)\n",
    "\n",
    "        # Step 5: Predict on test set\n",
    "        y_pred = predict_logistic_regression(X_test, theta)\n",
    "\n",
    "        # Step 6: Compute misclassification rate\n",
    "        misclassification_rate = 1 - accuracy_score(y_test, y_pred)\n",
    "        misclassification_rates.append(misclassification_rate)\n",
    "\n",
    "    # Step 7: Compute average misclassification rate\n",
    "    avg_misclassification_rate = np.mean(misclassification_rates)\n",
    "    return avg_misclassification_rate\n",
    "\n",
    "# ===========================\n",
    "# 7. RUN EXPERIMENT\n",
    "# ===========================\n",
    "# Parameters\n",
    "n = 200  # Number of observations\n",
    "p = 20   # Number of features\n",
    "alpha = 0.1  # Desired Bayes error rate\n",
    "noise_rate = 0.2  # 20% of -1 class will be flipped to +1\n",
    "\n",
    "# Run experiment\n",
    "avg_misclassification_rate = calculate_avg_misclassification_rate(n, p, alpha, noise_rate)\n",
    "print(f\"Average test misclassification rate over 100 repetitions: {avg_misclassification_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8804001",
   "metadata": {},
   "source": [
    "### 100-Repetition Experiment with Robust Logistic Regression With Shift Parameter Estimation. 'L1 Lasso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd297c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test misclassification rate over 100 repetitions: 0.2143\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===========================\n",
    "# 1. GENERATE SYNTHETIC DATA\n",
    "# ===========================\n",
    "def generate_two_class_data(n, p, alpha):\n",
    "    \"\"\"\n",
    "    Generate a two-class dataset with a specified Bayes error rate.\n",
    "    \"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha)\n",
    "    c_alpha = (2 * z_alpha) / np.sqrt(p)\n",
    "    mu_plus = (c_alpha / 2) * np.ones(p)\n",
    "    mu_minus = -(c_alpha / 2) * np.ones(p)\n",
    "    cov = np.eye(p)\n",
    "    n_half = n // 2\n",
    "    X_plus = np.random.multivariate_normal(mu_plus, cov, n_half)\n",
    "    X_minus = np.random.multivariate_normal(mu_minus, cov, n_half)\n",
    "    X = np.vstack((X_plus, X_minus))\n",
    "    y = np.hstack((np.ones(n_half), -np.ones(n_half)))\n",
    "    df = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(p)])\n",
    "    df['y'] = y\n",
    "    return df\n",
    "\n",
    "# ===========================\n",
    "# 2. ADD LABEL NOISE FUNCTION\n",
    "# ===========================\n",
    "def add_label_noise(df, noise_rate):\n",
    "    \"\"\"\n",
    "    Introduce label noise by flipping a percentage of -1 class labels to +1.\n",
    "    \"\"\"\n",
    "    df_noisy = df.copy()\n",
    "    negative_class_indices = df_noisy[df_noisy['y'] == -1].index\n",
    "    num_to_flip = int(noise_rate * len(negative_class_indices))\n",
    "    flip_indices = np.random.choice(negative_class_indices, size=num_to_flip, replace=False)\n",
    "    df_noisy.loc[flip_indices, 'y'] = 1  # Flip labels from -1 to +1\n",
    "    return df_noisy\n",
    "\n",
    "# ===========================\n",
    "# 3. SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# 4. THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# 5. ROBUST LOGISTIC REGRESSION WITH SHIFT PARAMETER ESTIMATION\n",
    "# ===========================\n",
    "def train_robust_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with L1 penalty\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * np.sum(np.abs(gamma))\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "#             print(f\"Early stopping at epoch {epoch}: Loss improvement below {tol}\")\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return theta, gamma\n",
    "\n",
    "# ===========================\n",
    "# 6. PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust logistic regression model.\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "# ===========================\n",
    "# 7. CALCULATE AVERAGE TEST MISCLASSIFICATION RATE\n",
    "# ===========================\n",
    "def calculate_avg_misclassification_rate(n, p, alpha, noise_rate, lambda_, a, threshold_type=\"soft\", n_repetitions=100):\n",
    "    \"\"\"\n",
    "    Run multiple iterations of training/testing and compute the average misclassification rate.\n",
    "    \"\"\"\n",
    "    misclassification_rates = []\n",
    "\n",
    "    for _ in range(n_repetitions):\n",
    "        df = generate_two_class_data(n, p, alpha)\n",
    "        df_noisy = add_label_noise(df, noise_rate)\n",
    "\n",
    "        X = df_noisy.drop(columns=['y']).values\n",
    "        y = df_noisy['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Train robust logistic regression\n",
    "        theta, gamma = train_robust_logistic_regression(X_train, y_train, lr=0.1, epochs=2000, tol=1e-6, lambda_=lambda_, a=a, threshold_type=threshold_type)\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "\n",
    "        # Compute misclassification rate\n",
    "        misclassification_rate = 1 - accuracy_score(y_test, y_pred)\n",
    "        misclassification_rates.append(misclassification_rate)\n",
    "\n",
    "    # Compute average misclassification rate\n",
    "    avg_misclassification_rate = np.mean(misclassification_rates)\n",
    "    return avg_misclassification_rate\n",
    "\n",
    "# ===========================\n",
    "# 8. RUN EXPERIMENT\n",
    "# ===========================\n",
    "# Parameters\n",
    "n = 200  # Number of observations\n",
    "p = 20    # Number of features\n",
    "alpha = 0.1  # Desired Bayes error rate\n",
    "noise_rate = 0.2  # 20% of -1 class will be flipped to +1\n",
    "lambda_ = 0.1  # Threshold parameter\n",
    "a = 2.0  # Multiplicative factor for shift parameter estimation\n",
    "threshold_type = \"hard\"  # Choose \"soft\" or \"hard\"\n",
    "\n",
    "# Run experiment\n",
    "avg_misclassification_rate = calculate_avg_misclassification_rate(n, p, alpha, noise_rate, lambda_, a, threshold_type)\n",
    "print(f\"Average test misclassification rate over 100 repetitions: {avg_misclassification_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770436ee",
   "metadata": {},
   "source": [
    "### 100-Repetition Experiment with Robust Logistic Regression With Shift Parameter Estimation. 'Elastic Net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84c1c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test misclassification rate over 100 repetitions: 0.1997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===========================\n",
    "# 1. GENERATE SYNTHETIC DATA\n",
    "# ===========================\n",
    "def generate_two_class_data(n, p, alpha):\n",
    "    \"\"\"Generate a two-class dataset with a specified Bayes error rate.\"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha)\n",
    "    c_alpha = (2 * z_alpha) / np.sqrt(p)\n",
    "    mu_plus = (c_alpha / 2) * np.ones(p)\n",
    "    mu_minus = -(c_alpha / 2) * np.ones(p)\n",
    "    cov = np.eye(p)\n",
    "    n_half = n // 2\n",
    "    X_plus = np.random.multivariate_normal(mu_plus, cov, n_half)\n",
    "    X_minus = np.random.multivariate_normal(mu_minus, cov, n_half)\n",
    "    X = np.vstack((X_plus, X_minus))\n",
    "    y = np.hstack((np.ones(n_half), -np.ones(n_half)))\n",
    "    df = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(p)])\n",
    "    df['y'] = y\n",
    "    return df\n",
    "\n",
    "# ===========================\n",
    "# 2. ADD LABEL NOISE FUNCTION\n",
    "# ===========================\n",
    "def add_label_noise(df, noise_rate):\n",
    "    \"\"\"Introduce label noise by flipping a percentage of -1 class labels to +1.\"\"\"\n",
    "    df_noisy = df.copy()\n",
    "    negative_class_indices = df_noisy[df_noisy['y'] == -1].index\n",
    "    num_to_flip = int(noise_rate * len(negative_class_indices))\n",
    "    flip_indices = np.random.choice(negative_class_indices, size=num_to_flip, replace=False)\n",
    "    df_noisy.loc[flip_indices, 'y'] = 1\n",
    "    return df_noisy\n",
    "\n",
    "# ===========================\n",
    "# 3. SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# 4. THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# 5. ROBUST LOGISTIC REGRESSION WITH ELASTIC NET\n",
    "# ===========================\n",
    "def train_robust_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, alpha=0.5, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation and Elastic Net regularization.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "\n",
    "        # Elastic Net penalty gradient:\n",
    "        l1_grad = alpha * np.sign(theta)  # L1 (Lasso)\n",
    "        l2_grad = (1 - alpha) * theta  # L2 (Ridge)\n",
    "\n",
    "        # Apply Elastic Net regularization\n",
    "        gradient_theta += lambda_ * (l1_grad + l2_grad)\n",
    "\n",
    "        # Update weights\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with Elastic Net penalty\n",
    "        l1_term = alpha * np.sum(np.abs(theta))\n",
    "        l2_term = (1 - alpha) * np.sum(theta**2)\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * (l1_term + l2_term)\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return theta, gamma\n",
    "\n",
    "# ===========================\n",
    "# 6. PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"Make predictions using trained robust logistic regression model.\"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "# ===========================\n",
    "# 7. CALCULATE AVERAGE TEST MISCLASSIFICATION RATE\n",
    "# ===========================\n",
    "def calculate_avg_misclassification_rate(n, p, alpha, noise_rate, lambda_, alpha_elastic, a, threshold_type=\"soft\", n_repetitions=100):\n",
    "    \"\"\"\n",
    "    Run multiple iterations of training/testing and compute the average misclassification rate.\n",
    "    \"\"\"\n",
    "    misclassification_rates = []\n",
    "\n",
    "    for _ in range(n_repetitions):\n",
    "        df = generate_two_class_data(n, p, alpha)\n",
    "        df_noisy = add_label_noise(df, noise_rate)\n",
    "\n",
    "        X = df_noisy.drop(columns=['y']).values\n",
    "        y = df_noisy['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Train robust logistic regression with Elastic Net\n",
    "        theta, gamma = train_robust_logistic_regression(X_train, y_train, lr=0.1, epochs=2000, tol=1e-6, lambda_=lambda_, alpha=alpha_elastic, a=a, threshold_type=threshold_type)\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "\n",
    "        # Compute misclassification rate\n",
    "        misclassification_rate = 1 - accuracy_score(y_test, y_pred)\n",
    "        misclassification_rates.append(misclassification_rate)\n",
    "\n",
    "    return np.mean(misclassification_rates)\n",
    "\n",
    "# ===========================\n",
    "# 8. RUN EXPERIMENT\n",
    "# ===========================\n",
    "# Parameters\n",
    "n = 200  # Number of observations\n",
    "p = 20    # Number of features\n",
    "alpha = 0.1  # Desired Bayes error rate\n",
    "noise_rate = 0.2  # 20% of -1 class will be flipped to +1\n",
    "lambda_ = 0.1  # Regularization parameter\n",
    "alpha_elastic = 0.1  # Mixing parameter (0 = Ridge, 1 = Lasso)\n",
    "a = 2.0  # Multiplicative factor for shift parameter estimation\n",
    "threshold_type = \"hard\"  # Choose \"soft\" or \"hard\"\n",
    "\n",
    "# Run experiment\n",
    "avg_misclassification_rate = calculate_avg_misclassification_rate(n, p, alpha, noise_rate, lambda_, alpha_elastic, a, threshold_type)\n",
    "print(f\"Average test misclassification rate over 100 repetitions: {avg_misclassification_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb492fec",
   "metadata": {},
   "source": [
    "# Results Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89dabdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Noise Level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Misclassification Rate",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a9a0cc73-c757-4abb-98a7-198ef00ab17a",
       "rows": [
        [
         "0",
         "Normal Logistic Regression",
         "5% Noise",
         "0.1562"
        ],
        [
         "1",
         "Normal Logistic Regression",
         "10% Noise",
         "0.1803"
        ],
        [
         "2",
         "Normal Logistic Regression",
         "20% Noise",
         "0.2285"
        ],
        [
         "3",
         "Robust Logistic Regression (L1 Lasso)",
         "No Noise",
         "0.1512"
        ],
        [
         "4",
         "Robust Logistic Regression (L1 Lasso)",
         "10% Noise",
         "0.174"
        ],
        [
         "5",
         "Robust Logistic Regression (L1 Lasso)",
         "20% Noise",
         "0.2177"
        ],
        [
         "6",
         "Robust Logistic Regression (Elastic Net)",
         "5% Noise",
         "0.1362"
        ],
        [
         "7",
         "Robust Logistic Regression (Elastic Net)",
         "10% Noise",
         "0.1573"
        ],
        [
         "8",
         "Robust Logistic Regression (Elastic Net)",
         "20% Noise",
         "0.1937"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Noise Level</th>\n",
       "      <th>Misclassification Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal Logistic Regression</td>\n",
       "      <td>5% Noise</td>\n",
       "      <td>0.1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Normal Logistic Regression</td>\n",
       "      <td>10% Noise</td>\n",
       "      <td>0.1803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Normal Logistic Regression</td>\n",
       "      <td>20% Noise</td>\n",
       "      <td>0.2285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robust Logistic Regression (L1 Lasso)</td>\n",
       "      <td>No Noise</td>\n",
       "      <td>0.1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robust Logistic Regression (L1 Lasso)</td>\n",
       "      <td>10% Noise</td>\n",
       "      <td>0.1740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Robust Logistic Regression (L1 Lasso)</td>\n",
       "      <td>20% Noise</td>\n",
       "      <td>0.2177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Robust Logistic Regression (Elastic Net)</td>\n",
       "      <td>5% Noise</td>\n",
       "      <td>0.1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Robust Logistic Regression (Elastic Net)</td>\n",
       "      <td>10% Noise</td>\n",
       "      <td>0.1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Robust Logistic Regression (Elastic Net)</td>\n",
       "      <td>20% Noise</td>\n",
       "      <td>0.1937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model Noise Level  \\\n",
       "0                Normal Logistic Regression    5% Noise   \n",
       "1                Normal Logistic Regression   10% Noise   \n",
       "2                Normal Logistic Regression   20% Noise   \n",
       "3     Robust Logistic Regression (L1 Lasso)    No Noise   \n",
       "4     Robust Logistic Regression (L1 Lasso)   10% Noise   \n",
       "5     Robust Logistic Regression (L1 Lasso)   20% Noise   \n",
       "6  Robust Logistic Regression (Elastic Net)    5% Noise   \n",
       "7  Robust Logistic Regression (Elastic Net)   10% Noise   \n",
       "8  Robust Logistic Regression (Elastic Net)   20% Noise   \n",
       "\n",
       "   Misclassification Rate  \n",
       "0                  0.1562  \n",
       "1                  0.1803  \n",
       "2                  0.2285  \n",
       "3                  0.1512  \n",
       "4                  0.1740  \n",
       "5                  0.2177  \n",
       "6                  0.1362  \n",
       "7                  0.1573  \n",
       "8                  0.1937  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for Normal Logistic Regression\n",
    "normal_data = {\n",
    "    \"Noise Level\": [\"5% Noise\", \"10% Noise\", \"20% Noise\"],\n",
    "    \"Misclassification Rate\": [0.1562, 0.1803, 0.2285]\n",
    "}\n",
    "\n",
    "# Data for Robust Logistic Regression with L1 Lasso\n",
    "robust_data = {\n",
    "    \"Noise Level\": [\"No Noise\", \"10% Noise\", \"20% Noise\"],\n",
    "    \"Misclassification Rate\": [0.1512, 0.1740, 0.2177]\n",
    "}\n",
    "\n",
    "# Data for Robust Logistic Regression with Elastic Net Regularizer\n",
    "elastic_data = {\n",
    "    \"Noise Level\": [\"5% Noise\", \"10% Noise\", \"20% Noise\"],\n",
    "    \"Misclassification Rate\": [0.1362, 0.1573, 0.1937]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "normal_df = pd.DataFrame(normal_data)\n",
    "robust_df = pd.DataFrame(robust_data)\n",
    "elastic_df = pd.DataFrame(elastic_data)\n",
    "\n",
    "# Add a column to distinguish between Normal and Robust\n",
    "normal_df[\"Model\"] = \"Normal Logistic Regression\"\n",
    "robust_df[\"Model\"] = \"Robust Logistic Regression (L1 Lasso)\"\n",
    "elastic_df[\"Model\"] = \"Robust Logistic Regression (Elastic Net)\"\n",
    "\n",
    "# Combine the two DataFrames\n",
    "results_df = pd.concat([normal_df, robust_df, elastic_df], ignore_index=True)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "results_df = results_df[[\"Model\", \"Noise Level\", \"Misclassification Rate\"]]\n",
    "\n",
    "# Print the DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de2b1a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
