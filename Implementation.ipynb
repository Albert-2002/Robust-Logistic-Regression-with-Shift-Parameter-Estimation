{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae26cda",
   "metadata": {},
   "source": [
    "### Robust Logistic Regression with Shift Parameter Estimation\n",
    "Python implementation of *Robust Logistic Regression with Shift Parameter Estimation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3ccc3",
   "metadata": {},
   "source": [
    "#### Experiments On Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d545aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_two_class_data(n, p, alpha):\n",
    "    \"\"\"\n",
    "    Generate synthetic two-class data from a multivariate normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Total number of observations (n/2 for each class).\n",
    "    - p: Number of features.\n",
    "    - alpha: Desired Bayes error rate.\n",
    "\n",
    "    Returns:\n",
    "    - df: A pandas DataFrame containing the generated data with columns 'x1', 'x2', ..., 'xp', 'y'.\n",
    "    \"\"\"\n",
    "    # Calculate c_alpha\n",
    "    z_alpha = norm.ppf(1 - alpha)  # Upper quantile of standard normal distribution\n",
    "    c_alpha = (2 * z_alpha) / np.sqrt(p)\n",
    "\n",
    "    # Mean vectors for the two classes\n",
    "    mu_plus = (c_alpha / 2) * np.ones(p)  # Mean for class y = +1\n",
    "    mu_minus = -(c_alpha / 2) * np.ones(p)  # Mean for class y = -1\n",
    "\n",
    "    # Covariance matrix (identity matrix)\n",
    "    cov = np.eye(p)\n",
    "\n",
    "    # Generate data for each class\n",
    "    n_half = n // 2\n",
    "    X_plus = np.random.multivariate_normal(mu_plus, cov, n_half)  # Class y = +1\n",
    "    X_minus = np.random.multivariate_normal(mu_minus, cov, n_half)  # Class y = -1\n",
    "\n",
    "    # Combine the data\n",
    "    X = np.vstack((X_plus, X_minus))\n",
    "    y = np.hstack((np.ones(n_half), -np.ones(n_half)))\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    columns = [f'x{i+1}' for i in range(p)]\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    df['y'] = y\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa13a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200  # Total number of observations\n",
    "p = 2    # Number of features\n",
    "alpha = 0.1  # Desired Bayes error rate\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_two_class_data(n, p, alpha)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81190160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_noise(df, noise_rate):\n",
    "    \"\"\"\n",
    "    Add label noise to the dataset by flipping the labels of a specified percentage of observations\n",
    "    from the -1 class to +1.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The pandas DataFrame containing the dataset.\n",
    "    - noise_rate: The percentage of observations from the -1 class to flip (e.g., 0.05 for 5%).\n",
    "\n",
    "    Returns:\n",
    "    - df_noisy: The DataFrame with added label noise.\n",
    "    \"\"\"\n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_noisy = df.copy()\n",
    "\n",
    "    # Identify the indices of the -1 class\n",
    "    negative_class_indices = df_noisy[df_noisy['y'] == -1].index\n",
    "\n",
    "    # Randomly select a portion of the -1 class to flip\n",
    "    num_to_flip = int(noise_rate * len(negative_class_indices))\n",
    "    flip_indices = np.random.choice(negative_class_indices, size=num_to_flip, replace=False)\n",
    "\n",
    "    # Flip the labels of the selected observations\n",
    "    df_noisy.loc[flip_indices, 'y'] = 1\n",
    "\n",
    "    return df_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0703a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4680a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rate = 0.05  # 5% of the -1 class will be flipped to +1\n",
    "df_noisy = add_label_noise(df, noise_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24112bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noisy['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756e04b",
   "metadata": {},
   "source": [
    "### 100-Repetition Experiment with Normal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b7369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Misclassification Rate: 0.2360\n",
      "Average Precision: 0.8731\n",
      "Average Recall: 0.7214\n",
      "Average F1-Score: 0.7879\n",
      "Average AUC: 0.8456\n",
      "Average Training Time (s): 0.0505\n",
      "Average Iterations Until Convergence: 649.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===========================\n",
    "# 1. GENERATE SYNTHETIC DATA\n",
    "# ===========================\n",
    "def generate_two_class_data(n, p, alpha):\n",
    "    \"\"\"\n",
    "    Generate a two-class dataset with a specified Bayes error rate.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Number of observations\n",
    "    - p: Number of features\n",
    "    - alpha: Desired Bayes error rate\n",
    "\n",
    "    Returns:\n",
    "    - df: A DataFrame containing feature columns and target column 'y'.\n",
    "    \"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha)\n",
    "    c_alpha = (2 * z_alpha) / np.sqrt(p)\n",
    "    mu_plus = (c_alpha / 2) * np.ones(p)\n",
    "    mu_minus = -(c_alpha / 2) * np.ones(p)\n",
    "    cov = np.eye(p)\n",
    "    n_half = n // 2\n",
    "    X_plus = np.random.multivariate_normal(mu_plus, cov, n_half)\n",
    "    X_minus = np.random.multivariate_normal(mu_minus, cov, n_half)\n",
    "    X = np.vstack((X_plus, X_minus))\n",
    "    y = np.hstack((np.ones(n_half), -np.ones(n_half)))\n",
    "    df = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(p)])\n",
    "    df['y'] = y\n",
    "    return df\n",
    "\n",
    "# ===========================\n",
    "# 2. ADD LABEL NOISE FUNCTION\n",
    "# ===========================\n",
    "def add_label_noise(df, noise_rate):\n",
    "    \"\"\"\n",
    "    Introduce label noise by flipping a percentage of -1 class labels to +1.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the dataset with class labels\n",
    "    - noise_rate: Proportion of negative class labels (-1) to flip\n",
    "\n",
    "    Returns:\n",
    "    - df_noisy: Modified dataset with noisy labels.\n",
    "    \"\"\"\n",
    "    df_noisy = df.copy()\n",
    "    negative_class_indices = df_noisy[df_noisy['y'] == -1].index\n",
    "    num_to_flip = int(noise_rate * len(negative_class_indices))\n",
    "    flip_indices = np.random.choice(negative_class_indices, size=num_to_flip, replace=False)\n",
    "    df_noisy.loc[flip_indices, 'y'] = 1  # Flip labels from -1 to +1\n",
    "    return df_noisy\n",
    "\n",
    "# ===========================\n",
    "# 3. SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# 4. IMPLEMENT LOGISTIC REGRESSION TRAINING WITH EARLY STOPPING\n",
    "# ===========================\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Train logistic regression using gradient descent with early stopping.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix (including bias column)\n",
    "    - y: Target values (-1 or +1)\n",
    "    - lr: Learning rate\n",
    "    - epochs: Max number of iterations\n",
    "    - tol: Tolerance for early stopping (if loss change < tol, stop training)\n",
    "\n",
    "    Returns:\n",
    "    - theta: Optimized weight parameters\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # Samples, features\n",
    "    theta = np.zeros(n)  # Initialize weights\n",
    "    prev_loss = float('inf')  # Initialize previous loss for tracking\n",
    "\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Compute margin (u = y * f(x))\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "\n",
    "        # Compute predictions using the sigmoid function\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # Compute gradient\n",
    "        gradient = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "\n",
    "        # Update weights\n",
    "        theta -= lr * gradient\n",
    "\n",
    "        # Compute current loss\n",
    "        loss = np.mean(np.log(1 + np.exp(-u)))\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "        # Early stopping condition: If loss improvement is too small, stop training\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "#             print(f\"Early stopping at epoch {epoch}: Loss improvement below {tol}\")\n",
    "            break\n",
    "\n",
    "        prev_loss = loss  # Update previous loss\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    return theta, epoch + 1, training_time\n",
    "\n",
    "# ===========================\n",
    "# 5. PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained logistic regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix\n",
    "    - theta: Trained model parameters\n",
    "\n",
    "    Returns:\n",
    "    - Predicted class labels (-1 or 1)\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)  # Convert to {-1,1}\n",
    "\n",
    "# ===========================\n",
    "# 6. CALCULATE AVERAGE TEST MISCLASSIFICATION RATE\n",
    "# ===========================\n",
    "def calculate_avg_metrics(n, p, alpha, noise_rate, n_repetitions=100):\n",
    "    \"\"\"\n",
    "    Run multiple iterations of training/testing and compute the average misclassification rate.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Number of observations\n",
    "    - p: Number of features\n",
    "    - alpha: Desired Bayes error rate\n",
    "    - noise_rate: Proportion of -1 class flipped to +1\n",
    "    - n_repetitions: Number of iterations for averaging results\n",
    "\n",
    "    Returns:\n",
    "    - avg_misclassification_rate: Mean test misclassification rate over `n_repetitions`.\n",
    "    \"\"\"\n",
    "    misclassification_rates = []\n",
    "    training_times = []\n",
    "    convergence_iters = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "\n",
    "    for _ in range(n_repetitions):\n",
    "        # Step 1: Generate clean dataset\n",
    "        df = generate_two_class_data(n, p, alpha)\n",
    "\n",
    "        # Step 2: Add label noise\n",
    "        df_noisy = add_label_noise(df, noise_rate)\n",
    "\n",
    "        # Step 3: Split into training and test sets\n",
    "        X = df_noisy.drop(columns=['y']).values\n",
    "        y = df_noisy['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Step 4: Train logistic regression with early stopping\n",
    "        theta, num_iters, train_time = train_logistic_regression(X_train, y_train, lr=0.1, epochs=5000, tol=1e-6)\n",
    "\n",
    "        # Step 5: Predict on test set\n",
    "        y_pred = predict_logistic_regression(X_test, theta)\n",
    "        y_scores = sigmoid(np.dot(X_test, theta))  # Probabilities for AUC\n",
    "\n",
    "        # Step 6: Compute misclassification rate\n",
    "        misclassification_rate = 1 - accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "        auc = roc_auc_score((y_test + 1) // 2, y_scores)  # Convert {-1,1} → {0,1} for AUC\n",
    "\n",
    "        misclassification_rates.append(misclassification_rate)\n",
    "        training_times.append(train_time)\n",
    "        convergence_iters.append(num_iters)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    # # Step 7: Compute average misclassification rate\n",
    "    # avg_misclassification_rate = np.mean(misclassification_rates)\n",
    "    return {\n",
    "        'avg_misclassification_rate': np.mean(misclassification_rates),\n",
    "        'avg_precision': np.mean(precisions),\n",
    "        'avg_recall': np.mean(recalls),\n",
    "        'avg_f1': np.mean(f1s),\n",
    "        'avg_auc': np.mean(aucs),\n",
    "        'avg_training_time': np.mean(training_times),\n",
    "        'avg_iterations': np.mean(convergence_iters)\n",
    "    }\n",
    "# ===========================\n",
    "# 7. RUN EXPERIMENT\n",
    "# ===========================\n",
    "# Parameters\n",
    "n = 200  # Number of observations\n",
    "p = 20   # Number of features\n",
    "alpha = 0.1  # Desired Bayes error rate\n",
    "noise_rate = 0.2  # 20% of -1 class will be flipped to +1\n",
    "\n",
    "results = calculate_avg_metrics(n, p, alpha, noise_rate, n_repetitions=100)\n",
    "\n",
    "print(f\"Average Test Misclassification Rate: {results['avg_misclassification_rate']:.4f}\")\n",
    "print(f\"Average Precision: {results['avg_precision']:.4f}\")\n",
    "print(f\"Average Recall: {results['avg_recall']:.4f}\")\n",
    "print(f\"Average F1-Score: {results['avg_f1']:.4f}\")\n",
    "print(f\"Average AUC: {results['avg_auc']:.4f}\")\n",
    "print(f\"Average Training Time (s): {results['avg_training_time']:.4f}\")\n",
    "print(f\"Average Iterations Until Convergence: {results['avg_iterations']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8804001",
   "metadata": {},
   "source": [
    "### 100-Repetition Experiment with Robust Logistic Regression With Shift Parameter Estimation. 'L1 Lasso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd297c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Misclassification Rate: 0.2283\n",
      "Average Precision: 0.8844\n",
      "Average Recall: 0.7267\n",
      "Average F1-Score: 0.7955\n",
      "Average AUC: 0.8563\n",
      "Average Training Time (s): 0.5821\n",
      "Average Iterations Until Convergence: 4984.08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===========================\n",
    "# 1. GENERATE SYNTHETIC DATA\n",
    "# ===========================\n",
    "def generate_two_class_data(n, p, alpha):\n",
    "    \"\"\"\n",
    "    Generate a two-class dataset with a specified Bayes error rate.\n",
    "    \"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha)\n",
    "    c_alpha = (2 * z_alpha) / np.sqrt(p)\n",
    "    mu_plus = (c_alpha / 2) * np.ones(p)\n",
    "    mu_minus = -(c_alpha / 2) * np.ones(p)\n",
    "    cov = np.eye(p)\n",
    "    n_half = n // 2\n",
    "    X_plus = np.random.multivariate_normal(mu_plus, cov, n_half)\n",
    "    X_minus = np.random.multivariate_normal(mu_minus, cov, n_half)\n",
    "    X = np.vstack((X_plus, X_minus))\n",
    "    y = np.hstack((np.ones(n_half), -np.ones(n_half)))\n",
    "    df = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(p)])\n",
    "    df['y'] = y\n",
    "    return df\n",
    "\n",
    "# ===========================\n",
    "# 2. ADD LABEL NOISE FUNCTION\n",
    "# ===========================\n",
    "def add_label_noise(df, noise_rate):\n",
    "    \"\"\"\n",
    "    Introduce label noise by flipping a percentage of -1 class labels to +1.\n",
    "    \"\"\"\n",
    "    df_noisy = df.copy()\n",
    "    negative_class_indices = df_noisy[df_noisy['y'] == -1].index\n",
    "    num_to_flip = int(noise_rate * len(negative_class_indices))\n",
    "    flip_indices = np.random.choice(negative_class_indices, size=num_to_flip, replace=False)\n",
    "    df_noisy.loc[flip_indices, 'y'] = 1  # Flip labels from -1 to +1\n",
    "    return df_noisy\n",
    "\n",
    "# ===========================\n",
    "# 3. SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# 4. THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# 5. ROBUST LOGISTIC REGRESSION WITH SHIFT PARAMETER ESTIMATION\n",
    "# ===========================\n",
    "def train_robust_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with L1 penalty\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * np.sum(np.abs(gamma))\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "#             print(f\"Early stopping at epoch {epoch}: Loss improvement below {tol}\")\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    return theta, epoch + 1, training_time, gamma\n",
    "\n",
    "# ===========================\n",
    "# 6. PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust logistic regression model.\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "# ===========================\n",
    "# 7. CALCULATE AVERAGE TEST MISCLASSIFICATION RATE\n",
    "# ===========================\n",
    "def calculate_avg_metrics_l1(n, p, alpha, noise_rate, lambda_, a, threshold_type=\"soft\", n_repetitions=100):\n",
    "    \"\"\"\n",
    "    Run multiple iterations of training/testing and compute the average misclassification rate.\n",
    "    \"\"\"\n",
    "    misclassification_rates = []\n",
    "    training_times = []\n",
    "    convergence_iters = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "\n",
    "    for _ in range(n_repetitions):\n",
    "        df = generate_two_class_data(n, p, alpha)\n",
    "        df_noisy = add_label_noise(df, noise_rate)\n",
    "\n",
    "        X = df_noisy.drop(columns=['y']).values\n",
    "        y = df_noisy['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Train robust logistic regression\n",
    "        theta, num_iters, train_time, gamma = train_robust_logistic_regression(X_train, y_train, lr=0.1, epochs=5000, tol=1e-6, lambda_=lambda_, a=a, threshold_type=threshold_type)\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "        y_scores = sigmoid(np.dot(X_test, theta))\n",
    "\n",
    "        # Compute misclassification rate\n",
    "        misclassification_rate = 1 - accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "        auc = roc_auc_score((y_test + 1) // 2, y_scores)  # {-1, 1} → {0, 1}\n",
    "\n",
    "        misclassification_rates.append(misclassification_rate)\n",
    "        training_times.append(train_time)\n",
    "        convergence_iters.append(num_iters)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    # # Compute average misclassification rate\n",
    "    # avg_misclassification_rate = np.mean(misclassification_rates)\n",
    "    return {\n",
    "        'avg_misclassification_rate': np.mean(misclassification_rates),\n",
    "        'avg_precision': np.mean(precisions),\n",
    "        'avg_recall': np.mean(recalls),\n",
    "        'avg_f1': np.mean(f1s),\n",
    "        'avg_auc': np.mean(aucs),\n",
    "        'avg_training_time': np.mean(training_times),\n",
    "        'avg_iterations': np.mean(convergence_iters)\n",
    "    }\n",
    "\n",
    "# ===========================\n",
    "# 8. RUN EXPERIMENT\n",
    "# ===========================\n",
    "# Parameters\n",
    "n = 200  # Number of observations\n",
    "p = 20    # Number of features\n",
    "alpha = 0.1  # Desired Bayes error rate\n",
    "noise_rate = 0.2  # 20% of -1 class will be flipped to +1\n",
    "lambda_ = 0.1  # Threshold parameter\n",
    "a = 2.0  # Multiplicative factor for shift parameter estimation\n",
    "threshold_type = \"hard\"  # Choose \"soft\" or \"hard\"\n",
    "\n",
    "# Run experiment\n",
    "# avg_misclassification_rate = calculate_avg_misclassification_rate(n, p, alpha, noise_rate, lambda_, a, threshold_type)\n",
    "# print(f\"Average test misclassification rate over 100 repetitions: {avg_misclassification_rate:.4f}\")\n",
    "\n",
    "# results = calculate_avg_misclassification_rate(n, p, alpha, noise_rate, lambda_, a, threshold_type, n_repetitions=100)\n",
    "\n",
    "# print(f\"Average Test Misclassification Rate: {results['avg_misclassification_rate']:.4f}\")\n",
    "# print(f\"Average Training Time (s): {results['avg_training_time']:.4f}\")\n",
    "# print(f\"Average Iterations Until Convergence: {results['avg_iterations']:.2f}\")\n",
    "\n",
    "results = calculate_avg_metrics_l1(n, p, alpha, noise_rate, lambda_, a, threshold_type, n_repetitions=100)\n",
    "\n",
    "print(f\"Average Test Misclassification Rate: {results['avg_misclassification_rate']:.4f}\")\n",
    "print(f\"Average Precision: {results['avg_precision']:.4f}\")\n",
    "print(f\"Average Recall: {results['avg_recall']:.4f}\")\n",
    "print(f\"Average F1-Score: {results['avg_f1']:.4f}\")\n",
    "print(f\"Average AUC: {results['avg_auc']:.4f}\")\n",
    "print(f\"Average Training Time (s): {results['avg_training_time']:.4f}\")\n",
    "print(f\"Average Iterations Until Convergence: {results['avg_iterations']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770436ee",
   "metadata": {},
   "source": [
    "### 100-Repetition Experiment with Robust Logistic Regression With Shift Parameter Estimation. 'Elastic Net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84c1c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Misclassification Rate: 0.2237\n",
      "Average Precision: 0.8790\n",
      "Average Recall: 0.7393\n",
      "Average F1-Score: 0.8011\n",
      "Average AUC: 0.8544\n",
      "Average Training Time (s): 0.0663\n",
      "Average Iterations Until Convergence: 867.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def update_gamma_elastic_net(y, X, theta, gamma, lambda_, alpha, lr_gamma=0.1):\n",
    "    \"\"\"\n",
    "    Update gamma using proximal gradient descent with Elastic Net regularization.\n",
    "    Applies one update step for all gamma_i.\n",
    "    \"\"\"\n",
    "    z = np.dot(X, theta)\n",
    "    margin = y * (z + gamma)\n",
    "    grad_gamma = -y * sigmoid(-margin)  # dL/dγ\n",
    "    gamma -= lr_gamma * grad_gamma\n",
    "\n",
    "    # Apply Elastic Net penalty\n",
    "    l1_component = alpha * lambda_\n",
    "    l2_component = (1 - alpha) * lambda_\n",
    "    gamma = np.sign(gamma) * np.maximum(np.abs(gamma) - l1_component * lr_gamma, 0) / (1 + l2_component * lr_gamma)\n",
    "\n",
    "    return gamma\n",
    "\n",
    "# ===========================\n",
    "# 1. GENERATE SYNTHETIC DATA\n",
    "# ===========================\n",
    "def generate_two_class_data(n, p, alpha):\n",
    "    \"\"\"Generate a two-class dataset with a specified Bayes error rate.\"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha)\n",
    "    c_alpha = (2 * z_alpha) / np.sqrt(p)\n",
    "    mu_plus = (c_alpha / 2) * np.ones(p)\n",
    "    mu_minus = -(c_alpha / 2) * np.ones(p)\n",
    "    cov = np.eye(p)\n",
    "    n_half = n // 2\n",
    "    X_plus = np.random.multivariate_normal(mu_plus, cov, n_half)\n",
    "    X_minus = np.random.multivariate_normal(mu_minus, cov, n_half)\n",
    "    X = np.vstack((X_plus, X_minus))\n",
    "    y = np.hstack((np.ones(n_half), -np.ones(n_half)))\n",
    "    df = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(p)])\n",
    "    df['y'] = y\n",
    "    return df\n",
    "\n",
    "# ===========================\n",
    "# 2. ADD LABEL NOISE FUNCTION\n",
    "# ===========================\n",
    "def add_label_noise(df, noise_rate):\n",
    "    \"\"\"Introduce label noise by flipping a percentage of -1 class labels to +1.\"\"\"\n",
    "    df_noisy = df.copy()\n",
    "    negative_class_indices = df_noisy[df_noisy['y'] == -1].index\n",
    "    num_to_flip = int(noise_rate * len(negative_class_indices))\n",
    "    flip_indices = np.random.choice(negative_class_indices, size=num_to_flip, replace=False)\n",
    "    df_noisy.loc[flip_indices, 'y'] = 1\n",
    "    return df_noisy\n",
    "\n",
    "# ===========================\n",
    "# 3. SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# 4. THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# 5. ROBUST LOGISTIC REGRESSION WITH ELASTIC NET\n",
    "# ===========================\n",
    "def train_robust_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, alpha=0.5, lr_gamma=0.1):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression with Elastic Net regularization on shift parameters gamma.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update theta (standard logistic regression with shift correction)\n",
    "        z = np.dot(X, theta)\n",
    "        margin = y * (z + gamma)\n",
    "        h = sigmoid(z + gamma)\n",
    "        grad_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "        theta -= lr * grad_theta\n",
    "\n",
    "        # Step 2: Update gamma via Elastic Net proximal update\n",
    "        gamma = update_gamma_elastic_net(y, X, theta, gamma, lambda_, alpha, lr_gamma=lr_gamma)\n",
    "\n",
    "        # Step 3: Compute full objective\n",
    "        margin = y * (np.dot(X, theta) + gamma)\n",
    "        logistic_loss = np.mean(np.log(1 + np.exp(-margin)))\n",
    "        l1_term = alpha * np.sum(np.abs(gamma))\n",
    "        l2_term = (1 - alpha) * np.sum(gamma**2)\n",
    "        loss = logistic_loss + lambda_ * (l1_term + 0.5 * l2_term)\n",
    "\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    return theta, epoch + 1, training_time, gamma\n",
    "\n",
    "# ===========================\n",
    "# 6. PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"Make predictions using trained robust logistic regression model.\"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "# ===========================\n",
    "# 7. CALCULATE AVERAGE TEST MISCLASSIFICATION RATE\n",
    "# ===========================\n",
    "def calculate_avg_metrics_elastic(n, p, alpha, noise_rate, lambda_, alpha_elastic, gamma_learning_rate ,n_repetitions=100):\n",
    "    \"\"\"\n",
    "    Run multiple iterations of training/testing and compute the average misclassification rate.\n",
    "    \"\"\"\n",
    "    misclassification_rates = []\n",
    "    training_times = []\n",
    "    convergence_iters = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "\n",
    "    for _ in range(n_repetitions):\n",
    "        df = generate_two_class_data(n, p, alpha)\n",
    "        df_noisy = add_label_noise(df, noise_rate)\n",
    "\n",
    "        X = df_noisy.drop(columns=['y']).values\n",
    "        y = df_noisy['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Train robust logistic regression with Elastic Net\n",
    "        theta, num_iters, train_time, gamma = train_robust_logistic_regression(X_train, y_train, lr=0.1, epochs=5000, tol=1e-6, lambda_=lambda_, alpha=alpha_elastic, lr_gamma=gamma_learning_rate)\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "        y_scores = sigmoid(np.dot(X_test, theta))\n",
    "\n",
    "        # Compute misclassification rate\n",
    "        misclassification_rate = 1 - accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "        auc = roc_auc_score((y_test + 1) // 2, y_scores)  # {-1, 1} → {0, 1}\n",
    "\n",
    "        misclassification_rates.append(misclassification_rate)\n",
    "        training_times.append(train_time)\n",
    "        convergence_iters.append(num_iters)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return {\n",
    "        'avg_misclassification_rate': np.mean(misclassification_rates),\n",
    "        'avg_precision': np.mean(precisions),\n",
    "        'avg_recall': np.mean(recalls),\n",
    "        'avg_f1': np.mean(f1s),\n",
    "        'avg_auc': np.mean(aucs),\n",
    "        'avg_training_time': np.mean(training_times),\n",
    "        'avg_iterations': np.mean(convergence_iters)\n",
    "    }\n",
    "\n",
    "# ===========================\n",
    "# 8. RUN EXPERIMENT\n",
    "# ===========================\n",
    "# Parameters\n",
    "n = 200  # Number of observations\n",
    "p = 20    # Number of features\n",
    "alpha = 0.1  # Desired Bayes error rate\n",
    "noise_rate = 0.2  # 20% of -1 class will be flipped to +1\n",
    "lambda_ = 0.1  # Regularization parameter\n",
    "alpha_elastic = 0.1  # Mixing parameter (0 = Ridge, 1 = Lasso)\n",
    "gamma_lr = 0.1\n",
    "\n",
    "# results = calculate_avg_misclassification_rate(n, p, alpha, noise_rate, lambda_, alpha_elastic, gamma_learning_rate=gamma_lr ,n_repetitions=100)\n",
    "\n",
    "# print(f\"Average Test Misclassification Rate: {results['avg_misclassification_rate']:.4f}\")\n",
    "# print(f\"Average Training Time (s): {results['avg_training_time']:.4f}\")\n",
    "# print(f\"Average Iterations Until Convergence: {results['avg_iterations']:.2f}\")\n",
    "\n",
    "results = calculate_avg_metrics_elastic(n, p, alpha, noise_rate, lambda_, alpha_elastic, gamma_learning_rate=gamma_lr, n_repetitions=100)\n",
    "\n",
    "print(f\"Average Test Misclassification Rate: {results['avg_misclassification_rate']:.4f}\")\n",
    "print(f\"Average Precision: {results['avg_precision']:.4f}\")\n",
    "print(f\"Average Recall: {results['avg_recall']:.4f}\")\n",
    "print(f\"Average F1-Score: {results['avg_f1']:.4f}\")\n",
    "print(f\"Average AUC: {results['avg_auc']:.4f}\")\n",
    "print(f\"Average Training Time (s): {results['avg_training_time']:.4f}\")\n",
    "print(f\"Average Iterations Until Convergence: {results['avg_iterations']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb492fec",
   "metadata": {},
   "source": [
    "# Results Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dabdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for Normal Logistic Regression\n",
    "normal_data = {\n",
    "    \"Noise Level\": [\"5% Noise\", \"10% Noise\", \"20% Noise\"],\n",
    "    \"Misclassification Rate\": [0.1562, 0.1803, 0.2285]\n",
    "}\n",
    "\n",
    "# Data for Robust Logistic Regression with L1 Lasso\n",
    "robust_data = {\n",
    "    \"Noise Level\": [\"5% Noise\", \"10% Noise\", \"20% Noise\"],\n",
    "    \"Misclassification Rate\": [0.1512, 0.1740, 0.2177]\n",
    "}\n",
    "\n",
    "# Data for Robust Logistic Regression with Elastic Net Regularizer\n",
    "elastic_data = {\n",
    "    \"Noise Level\": [\"5% Noise\", \"10% Noise\", \"20% Noise\"],\n",
    "    \"Misclassification Rate\": [0.1362, 0.1573, 0.1937]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "normal_df = pd.DataFrame(normal_data)\n",
    "robust_df = pd.DataFrame(robust_data)\n",
    "elastic_df = pd.DataFrame(elastic_data)\n",
    "\n",
    "# Add a column to distinguish between Normal and Robust\n",
    "normal_df[\"Model\"] = \"Normal Logistic Regression\"\n",
    "robust_df[\"Model\"] = \"Robust Logistic Regression (L1 Lasso)\"\n",
    "elastic_df[\"Model\"] = \"Robust Logistic Regression (Elastic Net)\"\n",
    "\n",
    "# Combine the two DataFrames\n",
    "results_df = pd.concat([normal_df, robust_df, elastic_df], ignore_index=True)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "results_df = results_df[[\"Model\", \"Noise Level\", \"Misclassification Rate\"]]\n",
    "\n",
    "# Print the DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de2b1a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
