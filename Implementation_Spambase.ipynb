{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 Iteration Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08110065170166547, 0.09558291093410576, 0.09196234612599563, 0.08761766835626361, 0.08472121650977549, 0.09992758870383778, 0.09775524981897177, 0.08761766835626361, 0.10644460535843592, 0.09051412020275162, 0.09847936278059377, 0.09196234612599563, 0.09413468501086164, 0.08182476466328747, 0.09051412020275162, 0.09196234612599563, 0.07748008689355534, 0.09123823316437363, 0.08254887762490948, 0.08327299058653148, 0.06806661839246919, 0.08761766835626361, 0.09051412020275162, 0.07965242577842147, 0.09703113685734976, 0.08327299058653148, 0.09051412020275162, 0.08979000724112962, 0.0861694424330196, 0.09703113685734976, 0.07096307023895732, 0.1028240405503259, 0.08689355539464161, 0.08906589427950762, 0.08254887762490948, 0.08472121650977549, 0.09413468501086164, 0.08037653874004347, 0.08472121650977549, 0.07603186097031134, 0.08037653874004347, 0.10209992758870379, 0.07965242577842147, 0.08761766835626361, 0.08689355539464161, 0.09920347574221577, 0.08906589427950762, 0.07603186097031134, 0.08254887762490948, 0.08327299058653148]\n",
      "Average misclassification rate over 50 runs L1 Lasso:\n",
      "Average: 0.0879 ± 0.0011 SE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "misclassification_rate_l1_lasso = []\n",
    "\n",
    "# Define column names based on the UCI Machine Learning Repository description\n",
    "column_names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n",
    "    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n",
    "    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\",\n",
    "    \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\",\n",
    "    \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\",\n",
    "    \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "    \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n",
    "    \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\",\n",
    "    \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\",\n",
    "    \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\", \"spam\"\n",
    "]\n",
    "\n",
    "file_path = \"C:/Users/91959/Desktop/CODE/Robust-Logistic-Regression-with-Shift-Parameter-Estimation/Robust Logistic Regression [DATA DIRECTORY]/Linear Case/spambase/spambase.data\"\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('spam', axis=1)  # Features\n",
    "y = df['spam']  # Target variable\n",
    "\n",
    "# Standardize the feature set (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "def introduce_label_noise(y, noise_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Introduces label noise by flipping a percentage of majority class labels to the minority class.\n",
    "\n",
    "    Args:\n",
    "        y (pd.Series): The target variable.\n",
    "        noise_percentage (float): The percentage of majority class labels to flip (e.g., 0.1 for 10%).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The target variable with label noise.\n",
    "    \"\"\"\n",
    "\n",
    "    value_counts = y.value_counts()\n",
    "    majority_class = value_counts.idxmax()\n",
    "    minority_class = value_counts.idxmin()\n",
    "\n",
    "    # print(\"Original class distribution:\")\n",
    "    # print(value_counts)\n",
    "\n",
    "    majority_indices = y[y == majority_class].index\n",
    "    num_noise = int(len(majority_indices) * noise_percentage)\n",
    "\n",
    "    noise_indices = np.random.choice(majority_indices, num_noise, replace=False)\n",
    "\n",
    "    y_noisy = y.copy()\n",
    "    y_noisy.loc[noise_indices] = minority_class\n",
    "\n",
    "    # print(\"\\nClass distribution after introducing noise:\")\n",
    "    # print(y_noisy.value_counts())\n",
    "\n",
    "    return y_noisy\n",
    "\n",
    "# Methods for LogReg\n",
    "# ===========================\n",
    "# SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# ROBUST LOGISTIC REGRESSION WITH SHIFT PARAMETER ESTIMATION\n",
    "# ===========================\n",
    "def train_robust_logistic_regression(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with L1 penalty\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * np.sum(np.abs(gamma))\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return theta, gamma\n",
    "\n",
    "# ===========================\n",
    "# PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust logistic regression model.\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "def print_average_with_se(data_list):\n",
    "    \"\"\"\n",
    "    Calculates and prints the average of a list with an error bar of ±SE.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): The list of numerical data.\n",
    "    \"\"\"\n",
    "\n",
    "    if not data_list:\n",
    "        print(\"Error: Input list is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        mean = statistics.mean(data_list)\n",
    "        stdev = statistics.stdev(data_list)\n",
    "        se = stdev / math.sqrt(len(data_list))\n",
    "\n",
    "        print(f\"Average: {mean:.4f} ± {se:.4f} SE\")\n",
    "\n",
    "    except statistics.StatisticsError:\n",
    "        print(\"Error: Cannot calculate standard deviation. List must contain at least two elements.\")\n",
    "    except TypeError:\n",
    "        print(\"Error: List elements must be numerical.\")\n",
    "\n",
    "def cross_validate_parameters(X, y, a_candidates, lambda_candidates, threshold_type='soft', n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to select the best parameters a and lambda_.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_a = None\n",
    "    best_lambda_ = None\n",
    "    best_error = float('inf')\n",
    "\n",
    "    for a in a_candidates:\n",
    "        for lambda_ in lambda_candidates:\n",
    "            print(a)\n",
    "            print(lambda_)\n",
    "            print('---------------------------------------')\n",
    "            cv_errors = []\n",
    "            for train_index, val_index in kf.split(X):\n",
    "                X_train, X_val = X[train_index], X[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                # Convert labels to -1 and 1\n",
    "                y_train_np = 2 * y_train - 1\n",
    "                y_val_np = 2 * y_val - 1\n",
    "\n",
    "                # Train robust logistic regression\n",
    "                theta, gamma = train_robust_logistic_regression(\n",
    "                    X_train, y_train_np, lr=0.1, epochs=2000, tol=1e-6, lambda_=lambda_, a=a, threshold_type=threshold_type\n",
    "                )\n",
    "\n",
    "                # Predict on validation set\n",
    "                y_pred = predict_robust_logistic_regression(X_val, theta)\n",
    "\n",
    "                # Compute misclassification rate\n",
    "                misclassification_rate = 1 - accuracy_score(y_val_np, y_pred)\n",
    "                cv_errors.append(misclassification_rate)\n",
    "\n",
    "            # Average misclassification rate over the folds\n",
    "            avg_error = np.mean(cv_errors)\n",
    "\n",
    "            # Update best parameters if current combination is better\n",
    "            if avg_error < best_error:\n",
    "                best_error = avg_error\n",
    "                best_a = a\n",
    "                best_lambda_ = lambda_\n",
    "\n",
    "    return best_a, best_lambda_, best_error\n",
    "\n",
    "# # Define candidate values for a and lambda_\n",
    "# a_candidates = [1, 2, 3, 4, 5, float('inf')]\n",
    "# lambda_candidates = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# # Perform cross-validation to select the best a and lambda_\n",
    "# best_a, best_lambda_, best_error = cross_validate_parameters(X, y, a_candidates, lambda_candidates, threshold_type='hard')\n",
    "\n",
    "# print(f\"Best a: {best_a}, Best lambda_: {best_lambda_}, Best CV error: {best_error}\")\n",
    "\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=i)\n",
    "\n",
    "    # Introduce label noise only to the training set\n",
    "    y_train_noisy = introduce_label_noise(y_train, noise_percentage=0.1)  # x% noise\n",
    "\n",
    "    # Convert labels to -1 and 1 for training and testing sets\n",
    "    y_train_np = 2 * y_train_noisy.values - 1  # Convert 0 → -1 and 1 → 1\n",
    "    y_test_np = 2 * y_test.values - 1          # Convert 0 → -1 and 1 → 1\n",
    "\n",
    "    # Train robust logistic regression on your dataset\n",
    "    # Best a: 1, Best lambda_: 10, Best CV error: 0.07998088089505735\n",
    "    theta, gamma = train_robust_logistic_regression(\n",
    "        X_train, y_train_np, lr=0.1, epochs=2000, tol=1e-6, lambda_=10, a=1, threshold_type=\"hard\"\n",
    "    )\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "\n",
    "    # Compute misclassification rate\n",
    "    misclassification_rate = 1 - accuracy_score(y_test_np, y_pred)\n",
    "    misclassification_rate_l1_lasso.append(misclassification_rate)\n",
    "\n",
    "print(misclassification_rate_l1_lasso)\n",
    "print(f\"Average misclassification rate over 50 runs L1 Lasso:\")\n",
    "print_average_with_se(misclassification_rate_l1_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08182476466328747, 0.07965242577842147, 0.09123823316437363, 0.08906589427950762, 0.09051412020275162, 0.09413468501086164, 0.08761766835626361, 0.09268645908761763, 0.10065170166545978, 0.09558291093410576, 0.09196234612599563, 0.08979000724112962, 0.09485879797248375, 0.08037653874004347, 0.08472121650977549, 0.09341057204923964, 0.08254887762490948, 0.08689355539464161, 0.09485879797248375, 0.08689355539464161, 0.07458363504706733, 0.08182476466328747, 0.08110065170166547, 0.07820419985517746, 0.10065170166545978, 0.08979000724112962, 0.09558291093410576, 0.08399710354815348, 0.08472121650977549, 0.0861694424330196, 0.07241129616220132, 0.10789283128167992, 0.08399710354815348, 0.08906589427950762, 0.08399710354815348, 0.09196234612599563, 0.08979000724112962, 0.08327299058653148, 0.07892831281679946, 0.07530774800868933, 0.08182476466328747, 0.10137581462708178, 0.08689355539464161, 0.08327299058653148, 0.0861694424330196, 0.10861694424330193, 0.08979000724112962, 0.08327299058653148, 0.08761766835626361, 0.08110065170166547]\n",
      "Average misclassification rate over 50 runs Elastic Net:\n",
      "Average: 0.0878 ± 0.0011 SE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "misclassification_rate_elastic_net = []\n",
    "\n",
    "# Define column names based on the UCI Machine Learning Repository description\n",
    "column_names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n",
    "    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n",
    "    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\",\n",
    "    \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\",\n",
    "    \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\",\n",
    "    \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "    \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n",
    "    \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\",\n",
    "    \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\",\n",
    "    \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\", \"spam\"\n",
    "]\n",
    "\n",
    "file_path = \"C:/Users/91959/Desktop/CODE/Robust-Logistic-Regression-with-Shift-Parameter-Estimation/Robust Logistic Regression [DATA DIRECTORY]/Linear Case/spambase/spambase.data\"\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('spam', axis=1)  # Features\n",
    "y = df['spam']  # Target variable\n",
    "\n",
    "# Standardize the feature set (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "def introduce_label_noise(y, noise_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Introduces label noise by flipping a percentage of majority class labels to the minority class.\n",
    "    \"\"\"\n",
    "    value_counts = y.value_counts()\n",
    "    majority_class = value_counts.idxmax()\n",
    "    minority_class = value_counts.idxmin()\n",
    "\n",
    "    majority_indices = y[y == majority_class].index\n",
    "    num_noise = int(len(majority_indices) * noise_percentage)\n",
    "\n",
    "    noise_indices = np.random.choice(majority_indices, num_noise, replace=False)\n",
    "\n",
    "    y_noisy = y.copy()\n",
    "    y_noisy.loc[noise_indices] = minority_class\n",
    "\n",
    "    return y_noisy\n",
    "\n",
    "# Methods for LogReg\n",
    "# ===========================\n",
    "# SIGMOID FUNCTION\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ===========================\n",
    "# THRESHOLDING FUNCTIONS FOR SHIFT PARAMETERS\n",
    "# ===========================\n",
    "def soft_threshold(u, lambda_, a):\n",
    "    \"\"\"Soft thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * np.minimum(u + lambda_, 0)\n",
    "\n",
    "def hard_threshold(u, lambda_, a):\n",
    "    \"\"\"Hard thresholding function for shift parameter estimation.\"\"\"\n",
    "    return a * u * (u <= -lambda_)\n",
    "\n",
    "# ===========================\n",
    "# ROBUST LOGISTIC REGRESSION WITH ELASTIC NET\n",
    "# ===========================\n",
    "def train_robust_logistic_regression_elastic(X, y, lr=0.01, epochs=1000, tol=1e-6, lambda_=1.0, alpha=0.5, a=2.0, threshold_type='soft'):\n",
    "    \"\"\"\n",
    "    Train robust logistic regression using gradient descent with shift parameter estimation and Elastic Net regularization.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    gamma = np.zeros(m)\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Step 1: Update weights theta using logistic regression with offset\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * (z - gamma)\n",
    "        h = sigmoid(z - gamma)\n",
    "        gradient_theta = np.dot(X.T, (h - (y + 1) / 2)) / m\n",
    "\n",
    "        # Elastic Net penalty gradient:\n",
    "        l1_grad = alpha * np.sign(theta)  # L1 (Lasso)\n",
    "        l2_grad = (1 - alpha) * theta  # L2 (Ridge)\n",
    "\n",
    "        # Apply Elastic Net regularization\n",
    "        gradient_theta += lambda_ * (l1_grad + l2_grad)\n",
    "\n",
    "        # Update weights\n",
    "        theta -= lr * gradient_theta\n",
    "\n",
    "        # Step 2: Update shift parameters gamma using thresholding\n",
    "        z = np.dot(X, theta)\n",
    "        u = y * z\n",
    "        if threshold_type == 'soft':\n",
    "            gamma = soft_threshold(u, lambda_, a)\n",
    "        elif threshold_type == 'hard':\n",
    "            gamma = hard_threshold(u, lambda_, a)\n",
    "        else:\n",
    "            raise ValueError(\"threshold_type must be 'soft' or 'hard'\")\n",
    "\n",
    "        # Step 3: Compute current loss with Elastic Net penalty\n",
    "        l1_term = alpha * np.sum(np.abs(theta))\n",
    "        l2_term = (1 - alpha) * np.sum(theta**2)\n",
    "        loss = np.mean(np.log(1 + np.exp(-u + gamma))) + lambda_ * (l1_term + l2_term)\n",
    "\n",
    "        # Early stopping condition\n",
    "        if abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return theta, gamma\n",
    "\n",
    "# ===========================\n",
    "# PREDICTION FUNCTION\n",
    "# ===========================\n",
    "def predict_robust_logistic_regression(X, theta):\n",
    "    \"\"\"\n",
    "    Make predictions using trained robust logistic regression model.\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, theta))\n",
    "    return np.where(probabilities >= 0.5, 1, -1)\n",
    "\n",
    "def print_average_with_se(data_list):\n",
    "    \"\"\"\n",
    "    Calculates and prints the average of a list with an error bar of ±SE.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        print(\"Error: Input list is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        mean = statistics.mean(data_list)\n",
    "        stdev = statistics.stdev(data_list)\n",
    "        se = stdev / math.sqrt(len(data_list))\n",
    "\n",
    "        print(f\"Average: {mean:.4f} ± {se:.4f} SE\")\n",
    "\n",
    "    except statistics.StatisticsError:\n",
    "        print(\"Error: Cannot calculate standard deviation. List must contain at least two elements.\")\n",
    "    except TypeError:\n",
    "        print(\"Error: List elements must be numerical.\")\n",
    "\n",
    "def cross_validate_parameters(X, y, a_candidates, lambda_candidates, alpha_candidates, threshold_type='soft', n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to select the best parameters a, lambda_, and alpha.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_a = None\n",
    "    best_lambda_ = None\n",
    "    best_alpha = None\n",
    "    best_error = float('inf')\n",
    "\n",
    "    for a in a_candidates:\n",
    "        for lambda_ in lambda_candidates:\n",
    "            for alpha in alpha_candidates:\n",
    "                print(a)\n",
    "                print(lambda_)\n",
    "                print(alpha)\n",
    "                print('--------------------------')\n",
    "                cv_errors = []\n",
    "                for train_index, val_index in kf.split(X):\n",
    "                    X_train, X_val = X[train_index], X[val_index]\n",
    "                    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                    # Convert labels to -1 and 1\n",
    "                    y_train_np = 2 * y_train - 1\n",
    "                    y_val_np = 2 * y_val - 1\n",
    "\n",
    "                    # Train robust logistic regression with Elastic Net\n",
    "                    theta, gamma = train_robust_logistic_regression_elastic(\n",
    "                        X_train, y_train_np, lr=0.1, epochs=2000, tol=1e-6, lambda_=lambda_, alpha=alpha, a=a, threshold_type=threshold_type\n",
    "                    )\n",
    "\n",
    "                    # Predict on validation set\n",
    "                    y_pred = predict_robust_logistic_regression(X_val, theta)\n",
    "\n",
    "                    # Compute misclassification rate\n",
    "                    misclassification_rate = 1 - accuracy_score(y_val_np, y_pred)\n",
    "                    cv_errors.append(misclassification_rate)\n",
    "\n",
    "                # Average misclassification rate over the folds\n",
    "                avg_error = np.mean(cv_errors)\n",
    "\n",
    "                # Update best parameters if current combination is better\n",
    "                if avg_error < best_error:\n",
    "                    best_error = avg_error\n",
    "                    best_a = a\n",
    "                    best_lambda_ = lambda_\n",
    "                    best_alpha = alpha\n",
    "\n",
    "    return best_a, best_lambda_, best_alpha, best_error\n",
    "\n",
    "# # Define candidate values for a, lambda_, and alpha\n",
    "# a_candidates = [1, 2, 3, 4, 5, float('inf')]\n",
    "# lambda_candidates = [0.01, 0.1, 1.0, 10.0]\n",
    "# alpha_candidates = [0.1, 0.5, 0.9]  # Values for alpha (balance between L1 and L2)\n",
    "\n",
    "# # Perform cross-validation to select the best a, lambda_, and alpha\n",
    "# best_a, best_lambda_, best_alpha, best_error = cross_validate_parameters(X, y, a_candidates, lambda_candidates, alpha_candidates, threshold_type='hard')\n",
    "\n",
    "# print(f\"Best a: {best_a}, Best lambda_: {best_lambda_}, Best alpha: {best_alpha}, Best CV error: {best_error}\")\n",
    "\n",
    "# Now run the 50 iteration experiment with the best parameters\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=i)\n",
    "\n",
    "    # Introduce label noise only to the training set\n",
    "    y_train_noisy = introduce_label_noise(y_train, noise_percentage=0.1)  # x% noise\n",
    "\n",
    "    # Convert labels to -1 and 1 for training and testing sets\n",
    "    y_train_np = 2 * y_train_noisy.values - 1  # Convert 0 → -1 and 1 → 1\n",
    "    y_test_np = 2 * y_test.values - 1          # Convert 0 → -1 and 1 → 1\n",
    "\n",
    "    # Train robust logistic regression on your dataset with the best parameters\n",
    "    # Best a: 1, Best lambda_: 0.01, Best alpha: 0.1, Best CV error: 0.08606642118680077\n",
    "    theta, gamma = train_robust_logistic_regression_elastic(\n",
    "        X_train, y_train_np, lr=0.1, epochs=2000, tol=1e-6, lambda_=0.01, alpha=0.1, a=1, threshold_type=\"hard\"\n",
    "    )\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = predict_robust_logistic_regression(X_test, theta)\n",
    "\n",
    "    # Compute misclassification rate\n",
    "    misclassification_rate = 1 - accuracy_score(y_test_np, y_pred)\n",
    "    misclassification_rate_elastic_net.append(misclassification_rate)\n",
    "\n",
    "print(misclassification_rate_elastic_net)\n",
    "print(f\"Average misclassification rate over 50 runs Elastic Net:\")\n",
    "print_average_with_se(misclassification_rate_elastic_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Noise Level': ['10% Noise', '20% Noise'],\n",
    "    'L1 Lasso': [\n",
    "        '0.0879 ± 0.0011 SE',\n",
    "        '0.0894 ± 0.0010 SE'\n",
    "    ],\n",
    "    'Elastic Net': [\n",
    "        '0.0878 ± 0.0011 SE',\n",
    "        '0.0881 ± 0.0010 SE'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the 'Noise Level' column as the index\n",
    "df.set_index('Noise Level', inplace=True)\n",
    "\n",
    "# Write to Excel file\n",
    "df.to_csv('Spambase_Results.csv')\n",
    "\n",
    "print(\"Excel file has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
